---
title: "Multi-omics guided pathway and network analysis of clinical metabolomics and proteomics data"
author:
  - name: "Christina Schmidt"
    affiliation:
      - Heidelberg University, Faculty of Medicine, and Heidelberg University Hospital, Institute for Computational Biomedicine, Heidelberg, Germany
  - name: "Thomas Naake"
    affiliation:
      - name: European Molecular Biology Laboratory, Meyerhofstrasse 1, 69117 Heidelberg, Germany
      - name: European Molecular Biology Laboratory, Notkestrasse 85, 22607 Hamburg, Germany
bibliography: bibliography.bib
format: docx
---

<!-- revise abstract -->

Metabolomics, the study of small molecules in biological systems, and proteomics, the study of proteins in biological systems, have become powerful tools for understanding biochemical pathways, discovering biomarkers, and elucidating disease mechanisms. This chapter provides a guide to performing metabolomics and proteomics data analysis in *R*,  focusing on pathway and network-based approaches. It covers essential steps in data processing, quality control (QC), differential expression analysis, multi-omics factor analysis (MOFA), statistical network analysis, and pathway analysis using prior knowledge. The methods outlined provide a framework for biomarker discovery and advancing systems-level understanding of disease processes using metabolomics and proteomics datasets.

keywords: Metabolomics, Proteomics, Processing, Quality control, Network analysis, Pathway, Knowledge graphs

```{r load_library}
#| message: false
#| echo: false

## install and load packages used for analysis
## BiocManager
if (!require("BiocManager", quietly = TRUE)) {
    install.packages("BiocManager")
    BiocManager::install(version = "3.20")
}

## cowplot
if (!require("cowplot", quietly = TRUE))
    BiocManager::install("cowplot")
library("cowplot")

## dplyr
if (!require("dplyr", quietly = TRUE))
    BiocManager::install("dplyr")
library("dplyr")

## fields
if (!require("fields", quietly = TRUE))
    BiocManager::install("fields")
library("fields")

## ggplot2
if (!require("ggplot2", quietly = TRUE))
    BiocManager::install("ggplot2")
library("ggplot2")

## ggplotify
if (!require("ggplotify", quietly = TRUE))
    BiocManager::install("ggplotify")
library("ggplotify")

## igraph
if (!require("igraph", quietly = TRUE))
    BiocManager::install("igraph")
library("igraph")

## magick
if (!require("magick", quietly = TRUE))
    BiocManager::install("magick")
library("magick")

## MatrixQCvis
if (!require("MatrixQCvis", quietly = TRUE))
    BiocManager::install("MatrixQCvis")
library("MatrixQCvis")

## MatrixQCvisUtils
if (!require("MatrixQCvisUtils", quietly = TRUE))
    BiocManager::install("MatrixQCvisUtils")
library("MatrixQCvisUtils")

## MetNet
if (!require("MetNet", quietly = TRUE))
    BiocManager::install("MetNet")
library("MetNet")

## MOFA2
if (!require("MOFA2", quietly = TRUE))
    BiocManager::install("MOFA2")
library("MOFA2")

## org.Hs.eg.db
if (!require("org.Hs.eg.db", quietly = TRUE))
    BiocManager::install("org.Hs.eg.db")
library("org.Hs.eg.db")

## SummarizedExperiment
if (!require("SummarizedExperiment", quietly = TRUE))
    BiocManager::install("SummarizedExperiment")
library("SummarizedExperiment")

## vsn
if (!require("vsn", quietly = TRUE))
    BiocManager::install("vsn")
library("vsn")

## devtools
if (!require("devtools", quietly = TRUE))
     BiocManager::install("devtools")

## MetaProViz
if (!require("MetaProViz", quietly = TRUE))
  devtools::install_github("https://github.com/saezlab/MetaProViz")
library("MetaProViz")
```

# Introduction
<!-- # Introduction to metabolomics and proteomics pathway and network analysis -->

Networks provide a powerful framework to study metabolomics and proteomics data by representing metabolites and protein as vertices and their relationships (e.g., correlations, biosynthetic enzymes) as edges. This approach enables insights into system-level changes and interactions between metabolites, proteomics and other omics layers. In this chapter, we will showcase the statistical programming language R's extensive ecosystem for metabolomics, proteomics and network analysis analysis, including packages like *igraph* [@Csardi2006], *MOFA* [@Argelaguet2018], and *decoupleR* [@BadiaIMompel2022]. MOFA, a statistical technique to identify latent factors of shared variation in multi-omics datasets, is used to integrate multiple data modalities that share the same sample set. We will use the results from the MOFA model to guide downstream analysis in pathway and network analysis. *decoupleR* uses biological signatures to calculate scores based on prior knowledge, offering the interpretation of the datasets by known biological networks and pathways.

In this chapter, we will distinguish between knowledge and experimental networks [@Amara2022]. Knowledge networks are constructed using existing biochemical and biological information, helping to interpret metabolomics and proteomics data within the framework of known pathways. For example, a metabolic reaction network represents a knowledge network, where metabolites act as vertices and their biochemical conversions are the edges. In contrast, experimental networks are directly derived from metabolomics and proteomics data, linking metabolites and proteins based on observed relationships like correlation or, in case of MS/MS metabolomics data, spectral similarity. Both types of networks can be explored using advanced statistical methods, graph analysis, and data-driven techniques to reveal meaningful patterns and connections within the dataset.

# Materials

We will use the dataset from @Gegner2024 to showcase functionality in *R* to analyse metabolomics and proteomics using network and pathway analysis tools. We present here an end-to-end pipeline starting with data import, data processing, quality assessment and control, multi-omics integration and downstream analysis to interpret the latent factors of the model. The datasets are proteomics and metabolomics measurements from fresh-frozen tumor and non-tumorous (adjacent) tissue of patients with lung adenocarcinoma. Metabolomics measurements were acquired using the MxP® Quant 500 kit (Biocrates) using LC-MS/MS and FIA-MS/MS measurements run on a UPLC I-class PLUS (Waters) system coupled to a SCIEX QTRAP® 6500 + mass spectrometry system in electrospray ionization (ESI) mode. Proteomics measurements were aquired using an Easy-nLC$^{\text{TM}}$ 1200 system (Thermo) coupled to a timsTOF Pro mass spectrometer (Bruker Daltonics). For further details on the analytical methods refer to @Gegner2024.

Due to space constraints, the code for all analyses will not be displayed in this chapter. Interested readers are refered to the complete and reproducible protocol available at
www.github.com/tnaake/MiMB_networks/R/chapter_network_multiomics.qmd.

# Methods


## Data processing and quality assessment and quality control (QA/QC)

Processing and quality assessment and quality control (QA/QC) are critical to ensure the integrity of metabolomics data. Steps typically include detection control of outliers, removal of batch effects, data normalization and transformation, and missing value imputation (optional). Each step requires careful parameter selection based on the dataset and experimental conditions. In the case of multi-omics analysis using MOFA, as presented here, it is paramount to ensure that all modalities share the same set of samples, with identical sample names and order, to facilitate accurate integration and analysis.

### Load the datasets, data wrangling

1.  Load the proteomics and metabolomics datasets via import functions from *MatrixQCvisUtils* (available via www.github.com/tnaake/MatrixQCvisUtils). Importing the XLSX sheets into *R* via the *MatrixQCvisUtils* package, creates *SummarizedExperiment* (@Morgan2024) objects. The *SummarizedExperiment* class stores the measured data (available via *assay*), next to the metadata associated to the samples (available via *colData*) and features (available via *rowData*).

```{r processing_load}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| results: false
## load metabolomics data
metabolomics <- biocrates("../data/12014_2024_9501_MOESM4_ESM.xlsx", 
    sheet = 1, colNames = FALSE)

## load proteomics data
proteomics <- maxquant(file = "../data/12014_2024_9501_MOESM3_ESM.xlsx",
    intensity = "LFQ", sheet = "ProteinGroups_Lung cancer cohor", type = "xlsx")
```

2.  After importing the datasets, check the following properties:
    -   correct representation of missing values (**NA**) in dataset,
    -   correct data dimension and structure,
    -   consistent number of samples and matching sample names between datasets,
    -   verify that the sample order is identical across the modalities,
    -   correct formatting of feature names, complete number of features for each dataset,
    -   complete and correctly formatted metadata associated to each dataset
3.  (optional) Depending on 2., identify the nature of missing features, harmonize sample names, correct feature names, harmonize metadata and check for completeness.

```{r processing_harmonize}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| results: false
## Loading the data from the supplemental material from @Gegner2024, the
## sample names are not correctly imported to the *R* session. In this
## step, the sample names are harmonized between the *metabolomics* and the
## *proteomics* dataset. In addition, when checking for the data
## dimension, we observe that the *proteomics* dataset contains
## information from two different extraction methods (number of samples of
## *proteomics*: `r ncol(proteomics)`, number of samples of
## *metabolomics*: `r ncol(metabolomics)`)). We also make sure, that the 
## metadata is harmonized between the two objects and that the 
## information on the tissue type is properly reflected in the metadata. 
## Metadata from Table 1 in @Gegner2024
## is added both to *metabolomics* and *proteomics*.

## data dimension and structure
dim(metabolomics)
dim(proteomics)

## sample names between datasets
##
## metabolomics
## add information on tissue type in column tissue
metabolomics$tissue <- ifelse(metabolomics$Tissue.type == "Tumor tissue", 
    "TU", "NAT")

## harmonize sample names
metabolomics$name <- paste(metabolomics$tissue, 
    rep(1:10, each = 2), sep = "_")
colnames(metabolomics) <- metabolomics$name

## make sure that colData only contains columns name and tissue
colData(metabolomics) <- colData(metabolomics)[, c("name", "tissue")]

## add information on patient_ID
metabolomics$patient_ID <- stringr::str_remove(metabolomics$name, 
    pattern = "NAT_|TU_") |>
    as.numeric()

##
## proteomics
## add information on tissue type in column tissue
proteomics$tissue <- ifelse(grepl(proteomics$name, pattern = "_TU_"), 
    "TU", "NAT")

## make sure that proteomics only contains samples from one extraction method
proteomics <- proteomics[, grep(colnames(proteomics), pattern = "ProtMet")]

## harmonize sample names
proteomics$name <- colnames(proteomics) |>
    stringr::str_remove(pattern = "LFQ.intensity.._KL_") |>
    stringr::str_remove(pattern = "ProtMet_IO_40min_DDA_") |>
    strsplit(split = "_") |>
    lapply(FUN = function(i) i[1]) |>
    unlist() |>
    stringr::str_remove(pattern = "l$")
df_names <- data.frame(
    rbind(
        c("1", "8JDLQY"),
        c("2", "K6R512"),
        c("3", "ZQ021J"),
        c("4", "KWF2HW"),
        c("5", "1FF2F9"),
        c("6", "IJ17TV"),
        c("7", "8U04FR"),
        c("8", "ZT9UTK"),
        c("9", "C5FQXS"),
        c("10", "36AT2O")
    ))
colnames(df_names) <- c("names_table", "names_proteomics")
proteomics$name <- paste(proteomics$tissue, 
    df_names[match(proteomics$name, df_names[["names_proteomics"]]), "names_table"],
    sep = "_")
colnames(proteomics) <- proteomics$name

## make sure that colData only contains columns name and tissue
colData(proteomics) <- colData(proteomics)[, c("name", "tissue")]

## add information on patient_ID
proteomics$patient_ID <- stringr::str_remove(proteomics$name, 
    pattern = "NAT_|TU_") |>
    as.numeric()

## make sure that metabolomics and proteomics have the same colnames
if (!all(colnames(metabolomics) %in% colnames(proteomics)))
    stop("Not all colnames(metabolomics) are found in colnames(proteomics)).")
if (!all(colnames(proteomics) %in% colnames(metabolomics)))
    stop("Not all colnames(proteomics) are found in colnames(metabolomics)).")

## order the colnames according to metabolomics
proteomics <- proteomics[, colnames(metabolomics)]
```

4.  (optional) Identify additional metadata, for example clinical parameters or translated IDs for metabolites (e.g. HMDB, KEGG, ChEBI ids) and proteins (e.g. Entrez, SYMBOL, Uniprot ids), from other sources. Add the metadata to the respective slots of the *SummarizedExperiment* object: *rowData* for feature-related and *colData* 
for sample-related metadata).

```{r processing_add_metadata}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| results: false
## For this dataset, additional metadata on clinical parameters are
## missing. We add information from Table 1 from @Gegner2024 to the
## *SummarizedExperiment* objects. For brievity, the steps can be followed
## in the corresponding GitHub repository of this chapter.

## metadata on patients (Table 1, from Gegner et al., 2024)
metadata <- data.frame(
    rbind(
        c("1", "61", "m", "ADC", "IB",  "0", "Ex- smoker",     "35", "yes"),
        c("2", "80", "f", "ADC", "IIB", "0", "Never- smoker",  "0",  "yes"),
        c("3", "62", "m", "ADC", "IB",  "1", "Ex- smoker",     "40", "yes"),
        c("4", "83", "f", "ADC", "IIB", "0", "Never- smoker",  "0",  "yes"),
        c("5", "56", "f", "ADC", "IIA", "0", "Current smoker", "30", "yes"),
        c("6", "77", "m", "ADC", "IIB", "0", "Ex- smoker",     "10", "no"), 
        c("7", "80", "m", "ADC", "IIB", "0", "Ex- smoker",     "2",  "no"),
        c("8", "72", "f", "ADC", "IIB", "0", "Current smoker", "50", "no"),
        c("9", "60", "m", "ADC", "IB",  "1", "Current smoker", "40", "no"),
        c("10", "57", "m", "ADC", "IB",  "0", "Ex- smoker",     "30", "no")
))
colnames(metadata) <- c("patient_ID", "age_at_diagnosis", "sex", "histology",
    "pstage", "ECOG", "smoking_status", "pack_years", "recurrence")
metadata$patient_ID <- as.numeric(metadata$patient_ID)
metadata$age_at_diagnosis <- as.numeric(metadata$age_at_diagnosis)
metadata$sex <- as.factor(metadata$sex)
metadata$histology <- as.factor(metadata$histology)
metadata$pstage <- as.factor(metadata$pstage)
metadata$ECOG <- as.factor(metadata$ECOG)
metadata$smoking_status <- as.factor(metadata$smoking_status)
metadata$pack_years <- as.numeric(metadata$pack_years)
metadata$recurrence <- as.factor(metadata$recurrence)

## link metadata to colData of metabolomics and proteomics
colData(metabolomics) <- colData(metabolomics) |>
    as.data.frame() |>
    dplyr::left_join(y = metadata, by = "patient_ID") |>
    DataFrame(row.names = colData(metabolomics)$name)
colData(metabolomics) <- colData(proteomics) |>
    as.data.frame() |>
    dplyr::left_join(y = metadata, by = "patient_ID") |>
    DataFrame(row.names = colData(proteomics)$name)

## add Symbol names to rowData of proteomics
uniprots <- Rkeys(org.Hs.egUNIPROT)
dict <- AnnotationDbi::select(org.Hs.eg.db, uniprots, "SYMBOL", "UNIPROT")
uniprot <- dict$UNIPROT
symbol <- dict$SYMBOL
names(symbol) <- uniprot
rowData(proteomics)$SYMBOL <- unlist(lapply(rowData(proteomics)$feature, 
    function(x) paste(symbol[strsplit(x, split = ";")[[1]]], collapse = ";")))
```

### Filtering of features in the datasets

Each dataset includes typically information on the reliability of features, which can be used to remove non-reliable features. This step improves the quality of the datasets. Filtering and removal of the features should be tailored to the study design and the characteristics of the dataset. While stricter filtering can reduce false positives, it may also remove true low-abundance features.

1.  Remove low or high abundance features. Low intensity features may be due to noise or below the limit of detection or quantification. High intensity may be above the limit of quantification, the linear dynamic range, or the calibration range. Removing these features prevents artifacts in downstream statistical analyses. For the targeted metabolomics dataset, ensure that the dataset only includes high-quality metabolite measurements. The MetIDQ output provides assessment of the quality of each measurement, encoded by a color code of each cell (dark blue: \< LOD (Limit of Detection; 3x signal to noise); light blue: \< LLOQ (Lower Limit of Quantification; 10x signal to noise) or \> ULOQ (Upper Limit of Quantification); green: valid; Yellow: Internal Standard out of range). Remove the low-quality features that do not meet the defined quantification criteria (*choice of user*). In this case, we will retain only those metabolites where at least 50% of the measurements fall within the quantification limits (valid). This filtering step improves data reliability and minimizing the impact of imprecise data points.

```{r processing_qc_metidq}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| results: false
source("utils.R")
valid_features <- metIDQ_get_high_quality_features(
    file = "../data/12014_2024_9501_MOESM4_ESM.xlsx", threshold = 0.5)
metabolomics <- metabolomics[valid_features, ]
```

2. Verify that all measurements fall within expected ranges (e.g., non-negative values for intensities). For measurements that fall outside the expected range, correct them, e.g., by setting these values to **NA** (missing data).

```{r}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| results: false
## metabolomics
assay(metabolomics)[which(assay(metabolomics) < 0)] <- NA

## proteomics
assay(proteomics)[which(assay(proteomics) < 0)] <- NA
```

3.  (optional) Remove those features that are only detected in a subset of samples (*choice of user*). High levels of missing data (**NA**) values can reduce statistical power. While imputation techniques can be applied, imputation of excessive numbers of missing values per feature can lead to unreliable results. Additionally, features identified in only a few biological replicates may lack reproducibility, impacting downstream analyses. Filtering criteria can be set on a global level, such as removing features absent in more than 50% of the samples, or based on metadata, such as removing features detected in fewer than two biological replicates per condition. More stringent thresholds, such as requiring detection in at least 50% of replicates within each condition, can further improve dataset reliability. The choice of filtering thresholds should be guided by domain-specific knowledge and the characteristics of the datasets. Overly strict filtering may remove biologically relevant proteins that are present only in specific subsets of samples, such as particular cell lines or patient groups. Filtering strategies should be carefully considered to maintain a balance between data quality and the retention of meaningful biological information.

```{r processing_qc_proteomics_remove_low_number_features}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| results: false
## we will remove the features that are detected in less than ten samples 
## metabolomics
valid_features <- apply(assay(metabolomics), MARGIN = 1, FUN = function(rows_i) !(sum(!is.na(rows_i)) < 10))
metabolomics <- metabolomics[valid_features, ]

## proteomics
valid_features <- apply(assay(proteomics), MARGIN = 1, FUN = function(rows_i) !(sum(!is.na(rows_i)) < 10))
proteomics <- proteomics[valid_features, ]
```

4.  Remove the features with high coefficient of variation (CV, *choice of user*). High variability in technical replicates or QC pools suggests inaccurate quantification. For example, features with CV \> 30-50% of raw intensities across technical replicates may be removed.

5. (optional) Remove features with a standard deviation of 0 as these features do not vary across samples and do not contain information for biological interpretation.

```{r processing_qc_sd}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| results: false
## metabolomics
valid_features <- apply(assay(metabolomics), MARGIN = 1, sd, na.rm = TRUE) > 0
metabolomics <- metabolomics[valid_features, ]

## proteomics
valid_features <- apply(assay(proteomics), MARGIN = 1, sd, na.rm = TRUE) > 0
proteomics <- proteomics[valid_features, ]
```

6.  For proteomics data, ensure that the dataset only includes high-quality protein measurements. In the proteomics dataset generated by MaxQuant [@Cox2008], peptide counts serve as an indicator of the reliability of protein identification. To ensure data quality, we will filter out proteins identified with low confidence (*choice of user*). In this case, we will remove any protein that has fewer than two peptides supporting its identification. This approach helps retain only well-supported protein identifications, enhancing the robustness of downstream analyses.

```{r processing_qc_peptide_counts}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| results: false
valid_features <- lapply(
    strsplit(rowData(proteomics)$peptide_counts_all, split = ";"), 
    function(i) max(as.numeric(i)) >= 2) |> 
    unlist()
valid_features[is.na(valid_features)] <- FALSE
proteomics <- proteomics[valid_features, ]
```

7.  For proteomics data, remove common contaminants by using a contaminant database (e.g., MaxQuant's contaminant list). Contaminants like keratins (from human skin), trypsin (from digestion), or bovine serum albumin (BSA) can interfere with interpretation of results.

8.  For the proteomics dataset, remove the features with reverse sequences as they do not correspond to real biological proteins. Reverse sequences refer to decoy sequences generated by reversing the amino acid sequence of real proteins. Reverse sequences are commonly used as negative controls in target-decoy strategies for false discovery rate estimation in mass-spectrometry-based proteomics.

```{r processing_qc_reverse}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| results: false
valid_features <- rowData(proteomics)$reverse != "+" 
valid_features[is.na(valid_features)] <- TRUE
proteomics <- proteomics[valid_features, ]
```

### Quality assessment and quality control (QA/QC)

The next step in the processing pipeline is to assess the quality of the data, including both the measurements and the metadata values, and to control for it.

The *MatrixQCvis* package [@Naake2022] performs QA/QC on *SummarizedExperiment* objects and offers users to perform these steps in an interactive *shiny* application.

1.  Launch the application via *shinyQC(metabolomics)* or *shinyQC(proteomics)* to begin the quality assessmet for the respective datasets.
2.  Check the number of measured and missing values per sample and per feature. The metric informs about the dynamic range of the acquisition. Differences between samples of an experiment may indicate differences in the dynamic range and/or in the sample content.
3.  Check the distribution of measurements per sample. The metric informs about the dynamic range of the acquisition. Differences between samples of an experiment may indicate differences in the dynamic range and/or in the sample content.
4.  Examine the mean-sd plot (Figure 1 A and B). The metric informs about the level of homoskedasticity of the dataset which may be important for parametric tests. Homoscedasticity is an assumption of parametric tests, e.g. t-tests. In case of sd-mean independence, the running median should be approximately horizontal.
5.  Examine the MA plots and Hoeffding's D statistic plot. The metric informs about systematic biases and variability in the data by taking into account the log2-fold change between two conditions (*M = log2(I_i) - log2(I_j)*) and the mean expression of two conditions (*A = 1/2 (log2(I_i) + log2(I_j))*), where *I_i* can be the intensity from one sample i and *I_j* the averaged intensities from a set of samples excluding sample *i*. Hoeffding's *D* statistic measures the dependency between *A* and *M*. It is a measure of the distance between *F(A, M)* and *G(A)H(M)*, where *F(A, M)* is the joint cumulative distribution function (CDF) of *A* and *M*, and *G* and *H* are marginal CDFs. The higher the value of *D*, the more dependent are *A* and *M*.
6.  Examine the empirical cumulative distribution function (ECDF) plots. The metric informs about the distribution, the skewness of a sample and helps to identify outliers.
7.  Examine the distance matrix between samples (Figure 1 C and D). The metric informs how similar or different samples are from one another based on the measured values. It informs about relationships and consistency between samples of the same group (e.g. treatment vs. control). The metric also helps to detect outliers and to identify batch effects.
8.  Examine dimension reduction, e.g. principal component analysis (PCA, Figure 1 E and F) and loadings plot. PCA preserves the variance between data points and aids in exploring relationships between samples and features. PCA is helpful in identifying structure in the dataset and in detecting outliers.
9.  Identify and remove any outliers based on the steps above. Reassess the data after the removal by repeating the steps 1.-8.
10. After reviewing the metrics 3.-9., identify the most appropriate methods for normalization, batch correction, transformation, and imputation for your datasets to ensure consistency and quality across samples and features.

```{r processing_qc_matrixqcvis_prepare_plots, fig.show="hide"}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| results: false
## A: meanSdPlot of raw values
gg_meanSd <- meanSdPlot(assay(proteomics), ranks = TRUE)$gg +
    theme(legend.position = "none")

## B: meanSdPlot of transformed values
gg_meanSd_log <- meanSdPlot(log(assay(proteomics)), ranks = TRUE)$gg + 
    theme(legend.position = "none")

## C: PCA of imputed raw values
pca <- assay(proteomics) |>
    imputeAssay(method = "MinDet") |>
    dimensionReduction(type = "PCA", 
        params = list(center = TRUE, scale = TRUE))
explainedVar <- explVar(pca[[2]], type = "PCA")
gg_pca <- dimensionReductionPlot(tbl = pca[[1]], se = proteomics, 
    color = "tissue", x_coord = "PC1", y_coord = "PC2", interactive = FALSE,
    explainedVar = explainedVar) +
    guides(color = guide_legend(title = "tissue"))

## D: PCA of imputed raw values
pca <- assay(proteomics) |>
    transformAssay(method = "log") |>
    imputeAssay(method = "MinDet") |>
    dimensionReduction(type = "PCA", 
        params = list(center = TRUE, scale = TRUE))
explainedVar <- explVar(pca[[2]], type = "PCA")
gg_pca_log <- dimensionReductionPlot(tbl = pca[[1]], se = proteomics, 
    color = "tissue", x_coord = "PC1", y_coord = "PC2", interactive = FALSE,
    explainedVar = explainedVar) +
    guides(color = guide_legend(title = "tissue"))

## E: distance matrix of raw values
d <- assay(proteomics) |>
    distShiny()
gg_dist <- distSample(d, proteomics, label = "tissue", 
    show_row_names = FALSE, title = "") |>
    as.ggplot()

## F: distance matrix of transformed values
d <- assay(proteomics) |>
    transformAssay(method = "log") |>
    distShiny()
gg_dist_log <- distSample(d, proteomics, label = "tissue", 
    show_row_names = FALSE, title = "") |>
    as.ggplot()
```

```{r figure_1}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| results: false
g <- plot_grid(gg_meanSd, gg_meanSd_log, gg_dist, gg_dist_log, 
    gg_pca, gg_pca_log, ncol = 2, labels = "AUTO")
ggsave(filename = "figure/figure1.jpg", plot = g, device = "jpg", dpi = 600)
```

### Normalization, batch correction, transformation, missing value imputation

Tools, such as MatrixQCvis [@Naake2022] can be used to explore data quality interactively and to optimize the choice of batch correction, normalization, or transformation methods. The *MatrixQCvis* package offers utility functions to perform normalization, batch reduction, transformation and imputation using a variety of methods (*normalizeAssay*, *batchCorrectionAssay*, *transformAssay*, *imputeAssay*).

1.  (optional) Normalize the dataset. Normalization adjusts for technical variability and ensures comparability of metabolite and protein levels across samples. The choice of method depends on the dataset’s variability and experimental design. Common methods are median normalization, quantile division normalization (division of measurements of each sample by the given quantile, e.g. 50% for median division), sum normalization (division of measurements of each sample by the sum of intensities column for that sample).
2.  (optional) Correct for batches. Batch effect correction is typically essential in multi-batch experiments acquired over a longer time, but may be not needed in smaller scale experiments. *ComBat*, for instance, requires specifying batch labels and optionally including covariates to retain biological variability. In case of linear effects of batches the batch effect can be removed via *limma*'s *removeBatchEffect* function and specifying the batch labels.
3.  Transform the dataset. For example, *log* transformation is commonly used to reduce skewness and heteroskedasticity, other methods, such as *log2*, *log10*, or variance stabilizing normalization (vsn) can be applied as well. After transformation, check the mean-sd plot to assess homoskedasticity and determine if the transformation has balanced the variance.
4.  (optional) Scale the features. Scaling standardizes the magnitude of intensities across features, making them comparable by putting them on a common scale (e.g., z-score).
5.  (optional) Impute the missing values (**NA**) of the datasets. Metabolomics and proteomics data acquired by mass spectrometry typically contain missing values. The choice of the imputation method depends on the mechanism of missingness (e.g., missing completely at random, missing at random, missing not at random). Various imputation techniques are available in *MatrixQCvis*, such as k-nearest neighbors imputation or imputation by minimum values. For a more in-depth discussion on imputation methods in metabolomics, refer to @Wei2018, and for proteomics, see @Lazar2016. Generally, missing values should be imputed with caution, as doing so requires assuming a model of missingness and can introduce bias. In many cases, downstream analyses can tolerate a certain level of missing values without significant impact on results. Tools such as *limma* and *MOFA* are designed to handle datasets with incomplete observations, making imputation unnecessary for many applications.

```{r processing_normalization}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| results: false
## In the case of the lung cancer adenocarcinoma dataset,
## we will apply quantile division normalization and log transformation to control
## for heteroskedasticity

## metabolomics
assay(metabolomics) <- assay(metabolomics) |>
    normalizeAssay(method = "quantile division", probs = 0.5,
        multiplyByNormalizationValue = TRUE) |>
    transformAssay(method = "log")

## proteomics
assay(proteomics) <- assay(proteomics) |>
    #normalizeAssay(method = "quantile division", probs = 0.5, 
    #    multiplyByNormalizationValue = TRUE) |>
    transformAssay(method = "log")
```

## Multi-omics integration using MOFA

MOFA [@Argelaguet2018] integrates omics modalities using a factor analysis to uncover latent factors that explain the shared variation across different datasets. Proper data wrangling and processing as discussed in previous sections (e.g., normalization, transformation) are critical steps in ensuring the reliability and interpretability of the results.

### Model building

1.  Define the modalities (omics data types) to be included in the model. Make sure that the modalities are properly normalized and transformed. (optional) Define group information. Group information refers to the different subsets or categories within the data that can be used to model variation across multiple omics layers. This could represent different experimental conditions, sample types (e.g., tumor vs. non-tumor), or other biological categories that might drive the variation in the data.

```{r mofa_create_mofa}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| results: false
# prepare data for MOFA
MOFAobject <- create_mofa(
    list(
        metabolomics = assay(metabolomics),
        proteomics = assay(proteomics)
))
```

2.  Define the parameters for training of the MOFA model (*choice of user*). Parameters control how data is preprocessed before training, such as whether views or groups are scaled to the same variance or centered (with a mean of zero). Parameters can define the structure of the MOFA model, such as the number of latent factors to infer, which can be guided by prior biological knowledge or model selection techniques. The user can also specify if a spike-and-slab prior on factor loadings is applied, which promotes sparsity in factor loadings by ensuring that only a few features have strong contributions while others are near zero. The user may apply the automatic relevance determination (ARD) priors on factor loadings, allowing the model to automatically determine which factors are relevant for each view. In addition, parameters can specify the training process of the model, including the mode of convergence of the algorithm, the use of stachastic inference, the seed for reproducibility, and the maximum number of iterations before the model training is interrupted.

3.  Add the model parameters to the untrained MOFA model and initiate the training of the model. The model will learn latent factors that explain the shared variation across the different omics datasets, based on the specified configurations and data characteristics.

```{r mofa_run_mofa}
#| eval: true
#| echo: false
#| message: false
#| warning: false 
#| results: false
## define the model parameterss
## data options
data_opts <- get_default_data_options(MOFAobject)
data_opts$scale_views <- FALSE ## default
data_opts$scale_groups <- FALSE ## default
data_opts$center_groups <- TRUE ## default

## model_options
model_opts <- get_default_model_options(MOFAobject)
model_opts$num_factors <- 5
model_opts$spikeslab_weights <- TRUE ## default
model_opts$ard_weights <- TRUE ## default

## training options
train_opts <- get_default_training_options(MOFAobject)
train_opts$convergence_mode <- "slow"
train_opts$stochastic <- FALSE
train_opts$seed <- 2025
train_opts$maxiter <- 10000

## prepara MOFA and run
MOFAobject <- prepare_mofa(
    object = MOFAobject,
    data_options = data_opts,
    model_options = model_opts,
    training_options = train_opts
)

## initialize MOFA model
#reticulate::py_install("mofapy2")
MOFAobject <- run_mofa(MOFAobject, 
    use_basilisk = FALSE)
```

4.  Inspect the sanity of the model output. Plot data overview and verify that the dimensions for each dataset are as expected. Calculate the correlation scores between the factors (Figure 2 A). The correlation coefficients between the factors should be low, indicating that each factor captures distinct variation in the data. If factors are found to be highly correlated, it may suggest that the model is not effectively separating the variation across the datasets. In such cases, consider reducing the number of trained factors. This will not only help in improving the interpretability of the model but also enhance the biological relevance of the factors by focusing on more distinct sources of variation.

```{r mofa_data_overview}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| results: false
#| fig-show: hide
## plot data overview
plot_data_overview(MOFAobject)
```

```{r mofa_correlation_between_factors}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| results: false
## sanity check: correlation between factors
png("figure/figure2_cor.png", width = 1500, height = 1500, res = 300)
plot_factor_cor(object = MOFAobject, method = "pearson")
dev.off()

#img <- png::readPNG("figure/figure2_tmp.png")
#g <- grid::rasterGrob(img, interpolate = TRUE)

#gg_cor <- ggplot() +
#  annotation_custom(g, xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = Inf) +
#  theme_void()
```

5.  Examine explained variance of the factors and per modality (Figure 2 B). The analysis shows the proportion of variance explained by each latent factor across different views (modalities) and groups (if applicable). Higher variance explained per factor means that the factor captures a significant portion of the variation in that particular view. If the first few factors explain most of the variance, it suggests that only a subset of factors drive the structure in the data. If certain factors explain more variance in one view compared to another, it suggests that the factor is more specific to that modality. Co-variance in most factors indicates a strong relationship between the data types. If plotted separately, differences in variance explained between groups may indicate distinct biological patterns or variation in data structure across conditions.

```{r mofa_variance_explained}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| results: false
#| fig-show: hide
## plot explained variance
png("figure/figure2_variance.png", width = 1500, height = 1500, res = 300)
plot_variance_explained(MOFAobject, x = "view", y = "factor", 
    plot_total = FALSE)## |>
    ##as.ggplot()
dev.off()

## plot variance explained per data modality.
plot_variance_explained(MOFAobject, x = "view", y = "factor",
    plot_total = TRUE)[[2]]

## Alternatively: Create a plot where the "activity" is shown.
## The activity is defined by a contribution of e.g. \>=30% variance explained
## in each layer per data modality, e.g. if a modality explains 28%
## variance of Factor1, then it will be set to inactive.
## plot active/inactive per data modality
.var <- calculate_variance_explained(MOFAobject)$r2_per_factor$group1
apply(.var, MARGIN = 1, 
        function(row_i) 100* row_i / sum(row_i)) |>
    apply(MARGIN = 2, 
        FUN = function(column_i) ifelse(column_i >= 30, "active", "inactive")) |>
    as.data.frame() |>
    tibble::rownames_to_column(var = "dataset") |>
    tidyr::pivot_longer(cols = starts_with("F"), names_to = "Factor") |>
    dplyr::mutate(dataset = factor(dataset),
        value = factor(value), 
        Factor = factor(Factor, levels = rownames(.var))) |>
    ggplot(aes(x = dataset, y = Factor, fill = value)) +
        geom_tile(stat = "identity", color = "black") +
        scale_fill_manual(values = c("active" = "#505050", "inactive" = "#eeeeee")) +
        xlab("") + ylab("")
```

### Model interpretation: Variance decomposition and analysis of factors

1.  Obtain sample covariates and cofactors and add to metadata. Metadata can
be retrieved from the *colData* slot of the *SummarizedExperiment* objects.

```{r mofa_add_covariates}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| results: false
## define covariates and add to metadata
covariates <- colData(metabolomics) |>
    as.data.frame() |>
    dplyr::select(-c(histology))
MOFAobject@samples_metadata <- dplyr::left_join(
    MOFAobject@samples_metadata, 
    covariates, by = c("sample" = "name"))
```

2.  Analyse the association between latent factors and metadata (e.g., clinical metadata, information on demographics, experimental condition, Figure 2 C). Compute correlations between the learned MOFA factors and the user-specific covariates (continous or categorical). Visualize and examine the correlation coefficients and p-values. The intensity and sign of the color indicate the strength and direction of the correlation. Positive correlations suggest that an increase in the covariate is associated with an increase in the factor values. Negative correlations suggest an inverse relationship.

```{r mofa_correlate_factors}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| results: false
#| fig-show: hide
## correlation with covariates
png("figure/figure2_cor_metadata.png", width = 1500, height = 1500, res = 300)
correlate_factors_with_covariates(MOFAobject, 
    covariates = c("tissue", "patient_ID", "age_at_diagnosis",
        "sex", "pstage", "ECOG", "smoking_status", "pack_years",
        "recurrence"), 
    plot = "r")
dev.off()
#img <- png::readPNG("figure/figure2_tmp.png")
#g <- grid::rasterGrob(img, interpolate = TRUE)

#gg_cor_metadata <- ggplot() +
#  annotation_custom(g, xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = Inf) +
#  theme_void()

## p-values of correlation with covariates
correlate_factors_with_covariates(MOFAobject, 
    covariates = c("tissue", "patient_ID", "age_at_diagnosis",
        "sex", "pstage", "ECOG", "smoking_status", "pack_years",
        "recurrence"), 
    plot = "log_pval", alpha = 0.05)
```

3.  Plot the separation of the samples by factors (Figure 2 D). Each factor arranges the samples along a one-dimensional axis centered at zero, where the absolute value is not important; only the relative positioning of the samples matters. Samples with different signs on the axis indicate opposite "effects" along the inferred variation axis, with larger absolute values suggesting stronger effects. The interpretation of these factors is similar to that of principal components in PCA, where each factor represents a direction of variation in the data, and the separation of samples along these factors provides insight into the underlying patterns.

```{r mofa_separation_samples_factors}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| results: false
#| fig-show: hide
allFactors <- get_factors(MOFAobject, factors = "all", as.data.frame = TRUE) |>
    tibble::as_tibble() |>
    dplyr::mutate(factor = gsub("Factor","F", factor))
allFactors <- dplyr::left_join(allFactors, covariates, by = c("sample" = "name"))
plotTab <- filter(allFactors, factor %in% c("F1", "F2", "F3", "F4", "F5")) |>
    tidyr::spread(key = factor, value = value)
plotTab$tissue <- as.character(plotTab$tissue)
plotTab$patient_ID <- as.factor(plotTab$patient_ID)
plotTab$sex <- as.factor(plotTab$sex)

## Factor 1 and 2
gg_factor1_factor2 <- ggplot(plotTab, aes(x = F1, y = F2)) +
    geom_point(aes(x = F1, y = F2, color = patient_ID, shape = tissue),
        stroke = 2, size = 4) +
    scale_shape_manual(values = c(TU = 21, NAT = 22)) +
    guides(color = guide_legend(ncol = 2)) +
    ggnewscale::new_scale_colour() +
    geom_point(aes(color = sex), size = 2) +
    guides(color = guide_legend(ncol = 2)) +
    xlab("Factor 1") + ylab("Factor 2") +
    theme_bw()

png("figure/figure2_factor1_factor2_simplified.png", width = 1500, height = 1500, res = 300)
ggplot(plotTab, aes(x = F1, y = F2)) +
    geom_point(aes(x = F1, y = F2, color = tissue),
        stroke = 2, size = 4) +
    scale_color_manual(values = c(NAT = "#1F78B4", TU = "#FF7F00")) +
    ggnewscale::new_scale_colour() +
    geom_point(aes(color = sex), size = 2) +
    scale_color_manual(values = c(f = "#33A02C", m = "#6A3D9A")) +
    xlab("Factor 1") + ylab("Factor 2") +
    theme_bw()
dev.off()

## Factor 1 and 3
gg_factor1_factor3 <- ggplot(plotTab, aes(x = F1, y = F3)) +
    geom_point(aes(color = patient_ID, shape = tissue), stroke = 2, size = 4) +
    scale_shape_manual(values = c(TU = 21, NAT = 22)) +
    guides(color = guide_legend(ncol = 2)) +
    ggnewscale::new_scale_colour() +
    geom_point(aes(color = sex), size = 2) +
    guides(color = guide_legend(ncol = 2)) +
    xlab("Factor 1") + ylab("Factor 3") +
    theme_bw()
```

4.  Analyse the weights of the factors (Figure 2 E). The weights represent how strongly each feature is associated with each factor. Features with little or no association to a factor will have values near zero, while those with a strong association will exhibit large absolute values. The sign of the weight indicates the direction of the effect: a positive weight means the feature has higher lievels in samples with positive factor values, while a negative weight indicates higher levels in samples with negative factor values.

```{r mofa_plot_top_weights}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| results: false
#| fig-show: hide

png("figure/figure2_top_weights.png", width = 1500, height = 1500, res = 300)
plot_top_weights(MOFAobject,
  view = "metabolomics",
  factor = 1,
  nfeatures = 10, ## number of features to highlight
  scale = TRUE,   ## scale weights from -1 to 1
  abs = FALSE     ## take the absolute value?
)
dev.off()
```

5.  Analyse the feature intensities across samples and relationships between features using the information from the MOFA model (Figure 2 F). The visualizations are helpful for identifying patterns clusters, assessing data quality and batch effects, detecting potential outliers, and understanding how different features, factors, or sample groups contribute to the overall variation in the dataset. The visualizations should be created for each factor of interest and for the most important (e.g. top 10 or 20) features per factor (*choice of user*).

```{r mofa_plot_data_heatmap}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| results: false
#| fig-show: hide
png("figure/figure2_heatmap.png", width = 1500, height = 1500, res = 300)
plot_data_heatmap(MOFAobject, view = "metabolomics", 
    factor = 1, features = 10,
    ## extra arguments that are passed to the `pheatmap` function
    cluster_rows = TRUE, cluster_cols = FALSE, show_rownames = TRUE, 
    show_colnames = FALSE, annotation_samples = "tissue")
dev.off()
 
plot_data_scatter(MOFAobject, view = "metabolomics", factor = 1, features = 6, 
    add_lm = TRUE, color_by = "tissue")
```


```{r figure_2}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| results: false
#| fig-show: hide
## load images
gg_cor <- image_read("figure/figure2_cor.png")
gg_variance <- image_read("figure/figure2_variance.png")
gg_cor_metadata <- image_read("figure/figure2_cor_metadata.png") |>
    image_crop(geometry = "1500x1250+0+200")
gg_factor1_factor2_simplified <- image_read("figure/figure2_factor1_factor2_simplified.png")
gg_top_weights <- image_read("figure/figure2_top_weights.png")
gg_heatmap <- image_read("figure/figure2_heatmap.png")

## convert images to ggplot objects
gg_cor <- ggdraw() + draw_image(gg_cor)
gg_variance <- ggdraw() + draw_image(gg_variance)
gg_cor_metadata <- ggdraw() + draw_image(gg_cor_metadata)
gg_factor1_factor2_simplified <- ggdraw() + draw_image(gg_factor1_factor2_simplified)
gg_top_weights <- ggdraw() + draw_image(gg_top_weights)
gg_heatmap <- ggdraw() + draw_image(gg_heatmap)

g <- plot_grid(gg_cor, gg_variance, 
                gg_cor_metadata, gg_factor1_factor2_simplified, 
                gg_top_weights, gg_heatmap,
     ncol = 2, rel_widths = c(1, 1), labels = "AUTO")
ggsave(filename = "figure/figure2.jpg", plot = g, device = "jpg", dpi = 600)
```


## Differential Expression Analysis

Differential analysis identifies metabolites with significant changes between conditions. Selecting appropriate statistical tests depends on the data distribution and experimental design.

### Statistical Testing
To systematically define which statistical tests to consider one can follow these steps:

1. Define which comparison(s) should be performed, which depends on the metainformation in your data as well as on the biological question. For ***one_vs_one (single comparison)*** with numerator= “Condition1” and denominator = “Condition2”. Here statistical tests such as t.test, wilcox.test, chisq.test, cor.test or lmFit can be used. For ***all_vs_one (multiple comparison)*** with numerator= c(“Condition2”, “Condition3”, “ConditionN”) and denominator = “Condition1” or ***all_vs_all (multiple comparison)*** with numerator= c(“Condition1”, “Condition2”, “ConditionN”) and denominator = c(“Condition1”, “Condition2”, “ConditionN”) multiple testing has to be performed for which statistical tests such as aov, welch, kruskal.test or lmFit can be used.

2. Check assumption of normality to understand if the data follow a normal- or not-normal distribution by for example performing a shapiro.test. For normally distributed data, t-tests or aov can be used, whilst for non-normally distributed data, non-parametric tests like wilcox.test rank-sum or kruskal.test are appropriate.

3. Check if the data are homoscedastic (equal variance) using Levene's test or Bartlett's test. If the data are not homoscedastic, consider using welch or wilcox.test.

4. Are there missing values (NAs) in the data? If this is the case tests such as lmFit needs to be used, as limma fits a linear model to the data and can handle missing values. Yet, the limma method assumes that the residuals of the linear model are approximately normally distributed.

Here we use the R package [MetaProViz](https://saezlab.github.io/MetaProViz/index.html) to perform differential analysis since it automatically performs both, the shapiro.test for normality and the Bartlett's test for homoscedasticity, and the choice between different statistical test [@Schmidt2025]. Importantly, MetaProViz also adjusts for multiple testing using Benjamini Hochberg, False Discovery Rate (FDR), Bonferroni or Holm [@Schmidt2025].

For simplicity, we will showcase an example performing a one-versus-one comparison of tumour versus normal:
```{r}
#| eval: true
#| echo: false
#| message: true
#| warning: false
#| results: false

# Differential metabolite analysis
DMA_Res <- MetaProViz::DMA(InputData= as.data.frame(t(SummarizedExperiment::assay(metabolomics)))%>% dplyr::mutate(across(everything(), ~ tidyr::replace_na(.x, 0))),
                           SettingsFile_Sample= as.data.frame(SummarizedExperiment::colData(metabolomics)),
                           SettingsInfo = c(Conditions="tissue", Numerator="TU" , Denominator = "NAT"),
                           SettingsFile_Metab = as.data.frame(SummarizedExperiment::rowData(metabolomics)),
                           StatPval ="t.test",
                           StatPadj="fdr",
                           PerformShapiro = TRUE,
                           PerformBartlett = TRUE,
                           SaveAs_Plot = NULL,
                           SaveAs_Table = NULL)
```

Now we can inspect the results starting with the Shapiro test results:
```{r}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| results: false

DMA_Res[["ShapiroTest"]][["DF"]]%>%
  as.data.frame()%>%
  dplyr::select(1:5)%>%
  kableExtra::kbl(caption = "`Shaprio.test` results.") %>%
  kableExtra::kable_classic(full_width = F, html_font = "Cambria", font_size = 12) 

DMA_Res[["ShapiroTest"]][["Plot"]]
```

Moreover, we can have a look at the results of the differential analysis 
```{r}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| results: false

DMA_Res[["DMA"]][["TU _vs_ NAT"]]%>%
  as.data.frame()%>%
  dplyr::select(1,23:28)%>%
  dplyr::arrange(p.adj)%>%
  dplyr::slice(1:10) %>% 
  kableExtra::kbl(caption = "DMA results Tumoru versus Normal.") %>%
  kableExtra::kable_classic(full_width = F, html_font = "Cambria", font_size = 12) 

DMA_Res[["VolcanoPlot"]]

```


The limma method assumes that the residuals of the linear model are approximately normally distributed. Check this by

A. Histogram of Residuals After fitting a linear model, plot the residuals to visually inspect normality:

```{r}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| results: false
# design <- model.matrix(~ factor(c(1,1,2,2)))  # Simple two-group design
# fit <- lmFit(expression_data, design)
# 
# # Get residuals
# residuals <- residuals(fit)
# 
# # Plot histogram
# hist(residuals, breaks = 50, main = "Histogram of Residuals", xlab = "Residuals")

```

B. Q-Q plot

```{r}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| results: false
# qqnorm(residuals)
# qqline(residuals, col = "red")}
```

### Visualization of differential analysis results

We have already seen the automatically produced Volcano plot of the differential analysis results. The x-axis represents the log2 fold change, while the y-axis represents the negative logarithm of the p-adjusted value. The volcano plot is a scatter plot that displays the relationship between the magnitude of change (log2 fold change) and the significance (p adjusted value) of each metabolite. Points above a certain threshold are considered statistically significant and hence it is important to set thresholds for significance (e.g. p.adj < 0.05) and log2 fold change(e.g. |log2FC| > 0.5).

With [MetaProViz](https://saezlab.github.io/MetaProViz/index.html) visualisation, we can easily add additional relevant information to the plot [@Schmidt2025]. First it can be relevant to add the metabolite name to the plot and colour code for metabolite class, which describes the metabolite class as provided by Biocrates. 
```{r}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| results: false

MetaProViz::VizVolcano(InputData=DMA_Res[["DMA"]][["TU _vs_ NAT"]]%>%
                                  tibble::column_to_rownames("Metabolite"),
                       SettingsInfo= c(color="class"),
                       SettingsFile_Metab= DMA_Res[["DMA"]][["TU _vs_ NAT"]]%>%
                                            tibble::column_to_rownames("Metabolite")%>%
                                            dplyr::select(24:28))
```

From the plot it is immediately clear that there are many metabolite classes displayed on the plot, which can make it difficult to interpret. Hence, we can create individual plots for each of the metabolite classes. Yet, as this would lead to many plots, we will select only three metabolite classes to showcase:
```{r}
#| eval: false
#| echo: false
#| message: false
#| warning: false
#| results: false

MetaProViz::VizVolcano(InputData=DMA_Res[["DMA"]][["TU _vs_ NAT"]]%>%
                                  tibble::column_to_rownames("Metabolite")%>%
                                  dplyr::filter(class %in% c("Ceramides", "Triacylglycerols", "Sphingolipids")),
                       SettingsInfo= c(individual="class"),
                       SettingsFile_Metab= DMA_Res[["DMA"]][["TU _vs_ NAT"]]%>%
                                            tibble::column_to_rownames("Metabolite")%>%
                                            dplyr::select(24:28),
                       SelectLab = NULL)
```


## Pathway and network analysis

<!-- !!! Christina, please add here !!! -->

--\> we can use Progeny to test factor weights of proteomics data and check for up/downregulation of pathways

### Pathway enrichment

Pathway enrichment links significant metabolites to biological pathways, providing context for observed changes. Ensure the metabolite identifiers match the database (e.g., HMDB IDs).

```{r}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| results: false
```


<!-- !!! Christina, please add here !!! -->

<!-- add other points??? -->

## Experimental network analysis using statistical methods

Statistical analysis uncovers pairwise relationships between metabolites and/or proteins within each modality. The analysis can be done to assess the co-occurence, association, or regulation of features within a dataset without any prior knowledge. The calculation of pairwise coefficients is often done via the Pearson or Spearman correlation calculation methods, which are useful for exploratory analysis to identify correlated feature pairs, suggesting potential biological relationships or shared pathways (guilt-by-association principle). Other methods may be more suitable depending on the research question, such as ARACNE (Algorithm for the Reconstruction of Accurate Cellular Networks), Bayesian network learning, CLR (Context Likelihood of Relatedness), GGM (Gaussian Graphical model),  linear regression using LASSO (Least Absolute Shrinkage and Selection Operator) regularization, partial Pearson correlation, Random Forest, or partial Spearman correlation. In the resulting network, vertices represent features, such as metabolites or proteins, while edges represent the statistical relationship between them, such as correlation coefficients.

We will showcase the creation of statistical networks based on the implemented functionality of the MetNet [@Naake2019] package.

1.  Choose an algorithm based on the data and biological question:

- *ARACNE* represents an mutual information (MI)-based approach that removes indirect interactions using data processing inequality. The algorithm should be used when aiming to extract the most relevant direct feature interactions from MI-based networks. It is more accurate than pure MI-based methods by reducing false positives by filtering indirect edges. The algorithm requires large sample sizes to estimate MI accurately.
- *Bayesian network learning*  infers regulatory networks by modeling conditional dependencies between genes using a probabilistic graphical model. The algorithm may be used when prior biological knowledge is available and probabilistic modeling is needed. The algorithm handles noise well, allows for causal inference, and is able to handle missing data. It may be, however, computationally expensive, requires strong prior assumptions, and may be sensitive to sample size.
- *CLR* uses MI to infer regulatory relationships, normalizing interactions for each feature. The algorithm works well when working with noisy or indirect regulatory influences. It improves MI-based inference by reducing false positives and performs well for global network structures. The algorithm assumes that regulatory interactions follow a Gaussian-like distribution, which may not always be true.
- *GGM* estimates a sparse precision matrix (inverse covariance) to infer direct interactions. The algorithm can be used when assuming a multivariate Gaussian structure in expression data. It identifies direct dependencies and avoids indirect correlations. The algorithm requires large sample sizes to obtain estimates accurately.
-  *Linear regression using LASSO regularization* identifies regulatory network by selecting a sparse set of predictor features for each target feature, thereby reducing the number of spurious edges in a network. The algorithm handles high-dimensional data well, reduces overfitting, and provides an interpretable sparse network. The algorithm assumes a linear relationship, may struggle with correlated predictors (features with similar expression profiles, and may be computationally expensive.
-   *Pearson correlation* measures linear relationships between feature expression/abundance profiles. The algorithm should be used when linear dependencies exist between feature expression/abundance levels. The algorithm is simple, computationally efficient, and easily interpretable. It does not capture non-linear relationships and is sensitive to noise.
-   *Partial Pearson correlation* identifies direct regulatory interactions by controlling for indirect effects. It can be used when confounding influences in a network must be corrected. The algorithm removes indirect correlations, providing clearer direct relationships. It requires large sample sizes to compute reliably.
-   *Random forest* infers regulatory network using feature importance scores from an ensemble of decision trees. The algorithm works well when dealing with non-linear relationships and heterogeneous data. It captures complex dependencies, is robust to noise, and does not assume a specific distribution. The algorithm can be computationally expensive and may overfit if not tuned properly.
-   *Spearman correlation* captures monotonic (rank-based) relationships between feature expression/abundance levels. The algorithm may be used when the data does not follow a normal distribution or when looking for ranked relationships. It is robust to outliers and captures non-linear relationships. The algorithm is less powerful for detecting direct gene interactions.
- *Partial Spearman correlation* is similar to partial Pearson correlation but for rank-based dependencies. The algorithm controls for confounding variables while dealing with non-linear relationships. It handles non-linearity and indirect interactions. It is less widely used in regulatory network inference and may be computationally demanding.

2.  Calculate the adjacency matrix using the `MetNet` [@Naake2019]. Specify the algorithms to be run. (optional) Prior to running the model, check if the model tolerates missing values (**NA**) and impute values if needed. (optional) Apply the Benjamini-Hochberg method for p-value adjustment for Pearson and Spearman correlation. The result of each algorithm is stored in a respective assay of the *AdjacencyMatrix* object, an extension of the *SummarizedExperiment* class. Note, that depending on the chosen algorithm, the adjacency matrices are not necessarily symmetric.

```{r network_statistical}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| results: false
stat_adj <- imputeAssay(assay(metabolomics), "MinDet") |>
    statistical(model = c("pearson", "spearman"), 
        p.adjust = "BH")
```

3.  Examine the distribution of similarity coefficients and p-values, if available.

```{r}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| results: false
#| fig-show: hide
## coefficients
coefs <- assay(stat_adj, "pearson_coef")[lower.tri(assay(stat_adj, "pearson_coef"))]
df <- data.frame(coefs = coefs)
    
png("figure/figure3_histogram_coef.png")
ggplot(df, aes(x = coefs)) +
    geom_histogram(fill = "steelblue", color = "black" ,alpha = 0.7) +
    labs(x = "Pearson correlation coefficients", y = "frequency") +
    theme_bw()
dev.off()
    
## p-values
pvals <- assay(stat_adj, "pearson_pvalue")[lower.tri(assay(stat_adj, "pearson_pvalue"))]
df <- data.frame(pvals = pvals)

png("figure/figure3_histogram_pvalues.png")
ggplot(df, aes(x = pvals)) +
    geom_histogram(fill = "#DC143C", color = "black" ,alpha = 0.7) +
    labs(x = "adjusted p-values", y = "frequency") +
    theme_bw()
dev.off()
```

4.  (optional) Retain highly correlating values (*choice of user*). Feature pairs with low
    coefficients will be regarded as noise and removed from the adjacency 
    matrix. 

```{r network_threshold}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| results: false
args_thr <- list(
    filter = "abs(pearson_coef) > 0.8 & pearson_pvalue < 0.05 & abs(spearman_coef) > 0.7 & spearman_pvalue < 0.05")
stat_adj_thr <- threshold(am = stat_adj, type = "threshold", args = args_thr)
```

5.  Create a graph from the consensus adjacency matrix. Specify the graph 
characteristics, e.g., if the graph should be unweighted *or* weighted or 
undirected *or* directed.

```{r network_graph}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| results: false
g <- graph_from_adjacency_matrix(assay(stat_adj_thr, "consensus"), 
    weighted = FALSE, mode = "undirected")
```

6.  (optional) Remove the singleton components and obtain the induced subgraph.

```{r network_graph_filtering}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| results: false
components_g <- igraph::components(g)
components_keep <- which(components_g$csize > 1)
V_keep <- names(components_g$membership)[
    components_g$membership %in% components_keep]
g_keep <- induced_subgraph(g, V_keep)
```

5.  Visualize network. Results from upstream analyses can be mapped to the network. As an example, the weights of latent factor 1 of the trained MOFA model can be used for colouring of the network vertices. Alternatively, we can use logFC of the differential expression analysis.

```{r network_visualisation}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| results: false
#| fig-show: hide
attributes_df <- get_weights(MOFAobject)$metabolomics |>
    as.data.frame() 
attributes_df <- cbind(name = rownames(get_weights(MOFAobject)$metabolomics), 
    attributes_df)
g_keep <- add_vertex_attributes(g = g_keep, attributes = attributes_df)

## create a color gradient: Blue (low) → White (neutral) → Red (high)
colors <- colorRampPalette(c("blue", "white", "red"))(100)  # 100 gradient steps

## normalize Factor1 values to fit within the color gradient index (1 to 100)
factor1_scaled <- round(scales::rescale(V(g_keep)$Factor1, to = c(1, 100)))

## assign colors based on scaled Factor1 values
V(g_keep)$color <- colors[factor1_scaled]

png("figure/figure3_network_factor1.png")
graphics::layout(matrix(c(1, 2), nrow = 2), heights = c(12, 1)) 
plot(g_keep, vertex.label = NA, vertex.size = 3, legend = TRUE)

min_val <- round(min(V(g_keep)$Factor1), digits = 2)
max_val <- round(max(V(g_keep)$Factor1), digits = 2 )

## plot the color legend
image.plot(legend.only = TRUE, add = FALSE, legend.lab = "MOFA Factor1 values", 
    horizontal = TRUE, col = colors, zlim = c(min_val, max_val), 
    legend.width = 1, legend.shrink = 0.2,
    axis.args = list(at = c(min_val, 0, max_val), 
    labels = c(min_val, "0", max_val), 
    cex.axis = 0.8))
dev.off()
```

6.  Choose and apply module determination algorithm (*choice of user*). To detect substructures in the graph, module determination algorithms can be applied on the graph. Common algorithms are (*igraph* implementation in brackets, see Table 1 for further details):
- *Community structure detection based on edge betweenness* (*cluster_edge_betweenness*) detects communities by iteratively removing edges with the highest betweenness centrality, splitting the network into distinct modules. It works well for small graphs, provides a clear hierarchical structure. The algorithm may be computationally expensive for large networks.
- *Community structure detection via greedy optimization of modularity* (*cluster_fast_greedy*) uses hierarchical agglomeration to merge vertices into communities, optimizing the modularity at each step. The algorithm works efficiently for large graphs, but 
is less effective for networks with overlapping communities.
- *Community structure detection algorithm based on interacting fluids* (*cluster_fluid_communities*) uses label propagation with a fixed number of communities, allowing them to expand dynamically based on node density. The algorithm is fast. The algorithm requires to predefine the number of communities and may yield inconsistent results across runs.
- *Community structure detection based on Infomap* (*cluster_infomap*) works by simulating random walks on the network and compressing the path description to reveal the best modular structure. The idea is that if a network has well-defined communities, a random walker will spend more time inside a community before jumping to another one. The algorithm captures this by trying to minimize the description length of the walk, effectively grouping vertices that are frequently visited together. The algorithm may be computationally demanding and is sensitive to parameter choices.
- *Community structure detection detection based on propagating labels* (*cluster_label_prop*) assigns labels to vertices, which spread iteratively based on majority voting until convergence. The algorithm scales linearly with the number of vertices and is, thus, works well on large networks. Due to the random nature of initiation, the results can be unstable and inconsistent for certain networks.
- *Community structure detection detection based on leading eigen* (*cluster_leading_eigen*) uses spectral clustering by computing the leading eigenvectors of a modularity matrix to find communities. The algorithm may be computationally expensive for large networks.
- *Community structure detection using the Leiden algorithm* (*cluster_leiden*) is a faster version of the Louvain algorithm and yields more accurate solutions. It iteratively refines partitions to ensure well-connected communities by optimizing either the modularity or the Constant Potts model, which does not suffer from the resolution limit. The algorithm may have difficulties finding small communities.
- *Community structure detection by multi-level optimization of modularity* (*cluster_louvain*) optimizes modularity by merging hierarchically small communities into larger ones in a greedy manner. The algorithm scales well to large networks. It may produce different results on each run and may has difficulties finding small communities.
- *community structure detection using the maximum modularity* (*cluster_optimal*) calculates the optimal community structure by maximizing the modularity over all possible partitions. The algorithm guarantees the best modularity score, but is computationally expensive for large network.
- *Community structure detection based on statistical mechanics* (*cluster_spinglass*) models community detection as an energy minimization problem via a spin-glass model and simulated annealing. The algorithm is computationally expensive for large networks and may be sensitive to parameter settings.
- *Community structure detection based on short random walks* (*cluster_walktrap*) simulates random walks on the network to detect community structure, assuming vertices within the same community are more likely to be visited in one walk. The algorithm may be slow for large networks.


<!-- ::: {.table} -->

Table 1: **Compatibility of community detection algorithms with network properties** 

| algorithm                   | undirected   | directed     | unweighted   | weighted     |
|-----------------------------|--------------|--------------|--------------|--------------|
| *cluster_edge_betweenness*  |$$\checkmark$$|$$\checkmark$$|$$\checkmark$$|$$\checkmark$$|
| *cluster_fast_greedy*       |$$\checkmark$$|  $$\times$$  |$$\checkmark$$|$$\checkmark$$|
| *cluster_fluid_communities* |$$\checkmark$$|  $$\times$$  |$$\checkmark$$|  $$\times$$  |
| *cluster_infomap*           |$$\checkmark$$|$$\checkmark$$|$$\checkmark$$|$$\checkmark$$|
| *cluster_label_prop*        |$$\checkmark$$|$$\checkmark$$|$$\checkmark$$|$$\checkmark$$|
| *cluster_leading_eigen*     |$$\checkmark$$|  $$\times$$  |$$\checkmark$$|$$\checkmark$$|
| *cluster_leiden*            |$$\checkmark$$|  $$\times$$  |$$\checkmark$$|$$\checkmark$$|
| *cluster_louvain*           |$$\checkmark$$|  $$\times$$  |$$\checkmark$$|$$\checkmark$$|
| *cluster_optimal*           |$$\checkmark$$|$$\checkmark$$|$$\checkmark$$|$$\checkmark$$|
| *cluster_spinglass*         |$$\checkmark$$|  $$\times$$  |$$\checkmark$$|$$\checkmark$$|
| *cluster_walktrap*          |$$\checkmark$$|  $$\times$$  |$$\checkmark$$|$$\checkmark$$|  

: width="100%"

: tbl-colwidths="28,17,17,21,17"

<!-- ::: -->


```{r network_cluster}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| results: false

attributes_df <- data.frame(
    infomap = membership(cluster_infomap(g_keep, e.weights = E(g_keep))))
attributes_df <- cbind(name = rownames(attributes_df), 
    attributes_df)
g_keep <- add_vertex_attributes(g = g_keep, attributes = attributes_df)

## assign colors based on scaled Factor1 values
V(g_keep)$color <- as.factor(V(g_keep)$infomap)

png("figure/figure3_network_infomap.png")
plot(g_keep, vertex.label = NA, vertex.size = 3)
dev.off()
```

7.  Downstream analysis on modules, e.g. decoupleR, composition of networks, etc.?

<!-- !!! Christina, please add here !!! -->


```{r figure_3}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| results: false
#| fig-show: hide
## load images
gg_hist_coef <- image_read("figure/figure3_histogram_coef.png")
gg_hist_pvalues <- image_read("figure/figure3_histogram_pvalues.png")
gg_network_factor1 <- image_read("figure/figure3_network_factor1.png")
gg_network_infomap <- image_read("figure/figure3_network_infomap.png")

## convert images to ggplot objects
gg_hist_coef <- ggdraw() + draw_image(gg_hist_coef)
gg_hist_pvalues <- ggdraw() + draw_image(gg_hist_pvalues)
gg_network_factor1 <- ggdraw() + draw_image(gg_network_factor1)
gg_network_infomap <- ggdraw() + draw_image(gg_network_infomap)

g <- plot_grid(gg_hist_coef, gg_hist_pvalues, 
                gg_network_factor1, gg_network_infomap, 
     ncol = 2, rel_widths = c(1, 1), labels = "AUTO")
ggsave(filename = "figure/figure3.jpg", plot = g, device = "jpg", dpi = 600)
```

## Analysis using biological knowledge, footprinting

<!-- !!! Christina, please add here !!! -->
ocEAn?? pathway analysis?

Footprinting connects metabolomics data to upstream regulatory elements. Parameters like pathway size and significance thresholds should be adjusted based on the dataset.

“Paradoxically, an omics gene cluster that is highly similar to gene sets in a reference database may be of lesser interest, since the cluster and its function have already been well characterized. Of greater interest are clusters of genes that have not been previously implicated, because it is precisely in these cases that new biological insights emerge. These less-studied cases may either show no statistically significant enrichment in the reference database, or they may return enrichments that are significant in terms of P value but not substantial in terms of gene set overlap. Here, an immediate next step is to explore the biological literature, as well as complementary datasets, to learn as much as possible about the genes in question. The goal is to mine knowledge pertinent to each gene and then use this knowledge to synthesize mechanistic hypotheses for a function that might be held in common by all or many genes in the set. This protracted process of discerning relevant findings from data and literature, then reasoning on this information to synthesize functional hypotheses, has not yet been widely automated but is one of the central tasks performed by a genome scientist.” Use of large language models (https://www.nature.com/articles/s41592-024-02525-x?utm_source=nmeth_etoc&utm_medium=email&utm_campaign=toc_41592_22_1&utm_content=20250114)

# Conclusion

This chapter demonstrated how to perform metabolomics data analysis in *R* using a network-based approach. By integrating tools for preprocessing, differential analysis, pathway and correlation analysis, and data integration, *R* provides a comprehensive framework for deriving biological insights from metabolomics and proteomics datasets. Parameters such as thresholds, normalization methods, and correction techniques must be carefully chosen to ensure the robustness and reliability of the results. 

# References


# Figure Captions/Table Captions/Tables


Figure 1: Assessment of log-transformation of proteomics dataset on quality metrics.
A: Mean-sd plot of raw data. B: Mean-sd plot of transformed data. 
Log-transformation controls heteroskedasticity.
C: Matrix of Euclidean distances between samples of raw data. 
D: Matrix of Euclidean distances between samples of transformed data.
E: PCA of raw data. F: PCA of transformed data. Log-transformation increases
variation on prinpical component 1 and 2.

Figure 2:
