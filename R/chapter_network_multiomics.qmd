---
title: "Multi-omics guided pathway and network analysis of clinical metabolomics and proteomics data"
author:
  - name: "Christina Schmidt"
    affiliation:
      - Heidelberg University, Faculty of Medicine, and Heidelberg University Hospital, Institute for Computational Biomedicine, Heidelberg, Germany
  - name: "Thomas Naake"
    affiliation:
      - name: European Molecular Biology Laboratory, Meyerhofstrasse 1, 69117 Heidelberg, Germany
      - name: European Molecular Biology Laboratory, Notkestrasse 85, 22607 Hamburg, Germany
bibliography: bibliography.bib
format: docx
editor_options: 
  chunk_output_type: console
---

<!-- revise abstract -->

Metabolomics, the study of small molecules in biological systems, and proteomics, the study of proteins in biological systems, have become powerful tools for understanding biochemical pathways, discovering biomarkers, and elucidating disease mechanisms. This chapter provides a guide to performing metabolomics and proteomics data analysis in *R*,  focusing on pathway and network-based approaches. It covers essential steps in data processing, quality control (QC), differential expression analysis, multi-omics factor analysis (MOFA), statistical network analysis, and pathway analysis using prior knowledge. The methods outlined provide a framework for biomarker discovery and advancing systems-level understanding of disease processes using metabolomics and proteomics datasets.

keywords: Metabolomics, Proteomics, Processing, Quality control, Network analysis, Pathway, Knowledge graphs

```{r load_library}
#| message: false
#| echo: false

## install and load packages used for analysis
## BiocManager
if (!require("BiocManager", quietly = TRUE)) {
    install.packages("BiocManager")
    BiocManager::install(version = "3.20")
}

## cowplot
if (!require("cowplot", quietly = TRUE))
    BiocManager::install("cowplot")
library("cowplot")

## dplyr
if (!require("dplyr", quietly = TRUE))
    BiocManager::install("dplyr")
library("dplyr")

## fields
if (!require("fields", quietly = TRUE))
    BiocManager::install("fields")
library("fields")

## ggplot2
if (!require("ggplot2", quietly = TRUE))
    BiocManager::install("ggplot2")
library("ggplot2")

## ggplotify
if (!require("ggplotify", quietly = TRUE))
    BiocManager::install("ggplotify")
library("ggplotify")

## igraph
if (!require("igraph", quietly = TRUE))
    BiocManager::install("igraph")
library("igraph")

## magick
if (!require("magick", quietly = TRUE))
    BiocManager::install("magick")
library("magick")

## MatrixQCvis
if (!require("MatrixQCvis", quietly = TRUE))
    BiocManager::install("MatrixQCvis")
library("MatrixQCvis")

## MatrixQCvisUtils
if (!require("MatrixQCvisUtils", quietly = TRUE))
    BiocManager::install("MatrixQCvisUtils")
library("MatrixQCvisUtils")

## MetNet
if (!require("MetNet", quietly = TRUE))
    BiocManager::install("MetNet")
library("MetNet")

## MOFA2
if (!require("MOFA2", quietly = TRUE))
    BiocManager::install("MOFA2")
library("MOFA2")

## org.Hs.eg.db
if (!require("org.Hs.eg.db", quietly = TRUE))
    BiocManager::install("org.Hs.eg.db")
library("org.Hs.eg.db")

## SummarizedExperiment
if (!require("SummarizedExperiment", quietly = TRUE))
    BiocManager::install("SummarizedExperiment")
library("SummarizedExperiment")

## vsn
if (!require("vsn", quietly = TRUE))
    BiocManager::install("vsn")
library("vsn")

## MetaProViz
if (!require("MetaProViz", quietly = TRUE))
  BiocManager::install("saezlab/MetaProViz")
library("MetaProViz")

## xlsx (used in utils.R)
if (!require("xlsx", quietly = TRUE))
    BiocManager::install("xlsx")

```

# Introduction
<!-- # Introduction to metabolomics and proteomics pathway and network analysis -->

Networks provide a powerful framework to study metabolomics and proteomics data by representing metabolites and protein as vertices and their relationships (e.g., correlations, biosynthetic enzymes) as edges. This approach enables insights into system-level changes and interactions between metabolites, proteomics and other omics layers. In this chapter, we will showcase the statistical programming language R's extensive ecosystem for metabolomics, proteomics and network analysis analysis, including packages like *igraph* [@Csardi2006], *MOFA* [@Argelaguet2018], and *decoupleR* [@BadiaIMompel2022]. MOFA, a statistical technique to identify latent factors of shared variation in multi-omics datasets, is used to integrate multiple data modalities that share the same sample set. We will use the results from the MOFA model to guide downstream analysis in pathway and network analysis. *decoupleR* uses biological signatures to calculate scores based on prior knowledge, offering the interpretation of the datasets by known biological networks and pathways.

In this chapter, we will distinguish between knowledge and experimental networks [@Amara2022]. Knowledge networks are constructed using existing biochemical and biological information, helping to interpret metabolomics and proteomics data within the framework of known pathways. For example, a metabolic reaction network represents a knowledge network, where metabolites act as vertices and their biochemical conversions are the edges. In contrast, experimental networks are directly derived from metabolomics and proteomics data, linking metabolites and proteins based on observed relationships like correlation or, in case of MS/MS metabolomics data, spectral similarity. Both types of networks can be explored using advanced statistical methods, graph analysis, and data-driven techniques to reveal meaningful patterns and connections within the dataset.

# Materials

We will use the dataset from @Gegner2024 to showcase functionality in *R* to analyse metabolomics and proteomics using network and pathway analysis tools. We present here an end-to-end pipeline starting with data import, data processing, quality assessment and control, multi-omics integration and downstream analysis to interpret the latent factors of the model. The datasets are proteomics and metabolomics measurements from fresh-frozen tumor and non-tumorous (adjacent) tissue of patients with lung adenocarcinoma. Metabolomics measurements were acquired using the MxP® Quant 500 kit (Biocrates) using LC-MS/MS and FIA-MS/MS measurements run on a UPLC I-class PLUS (Waters) system coupled to a SCIEX QTRAP® 6500 + mass spectrometry system in electrospray ionization (ESI) mode. Proteomics measurements were aquired using an Easy-nLC$^{\text{TM}}$ 1200 system (Thermo) coupled to a timsTOF Pro mass spectrometer (Bruker Daltonics). For further details on the analytical methods refer to @Gegner2024.

Due to space constraints, the code for all analyses will not be displayed in this chapter. Interested readers are refered to the complete and reproducible protocol available at
www.github.com/tnaake/MiMB_networks/R/chapter_network_multiomics.qmd.

# Methods


## Data processing and quality assessment and quality control (QA/QC)

Processing and quality assessment and quality control (QA/QC) are critical to ensure the integrity of metabolomics data. Steps typically include detection control of outliers, removal of batch effects, data normalization and transformation, and missing value imputation (optional). Each step requires careful parameter selection based on the dataset and experimental conditions. In the case of multi-omics analysis using MOFA, as presented here, it is paramount to ensure that all modalities share the same set of samples, with identical sample names and order, to facilitate accurate integration and analysis.

### Load the datasets, data wrangling

1.  Load the proteomics and metabolomics datasets via import functions from *MatrixQCvisUtils* (available via www.github.com/tnaake/MatrixQCvisUtils). Importing the XLSX sheets into *R* via the *MatrixQCvisUtils* package, creates *SummarizedExperiment* (@Morgan2024) objects. The *SummarizedExperiment* class stores the measured data (available via *assay*), next to the metadata associated to the samples (available via *colData*) and features (available via *rowData*).

```{r processing_load}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| results: false
## load metabolomics data
metabolomics <- biocrates("../data/12014_2024_9501_MOESM4_ESM.xlsx", 
    sheet = 1, colNames = FALSE)

## load proteomics data
proteomics <- maxquant(file = "../data/12014_2024_9501_MOESM3_ESM.xlsx",
    intensity = "LFQ", sheet = "ProteinGroups_Lung cancer cohor", type = "xlsx")
```

2.  After importing the datasets, check the following properties:
    -   correct representation of missing values (**NA**) in dataset,
    -   correct data dimension and structure,
    -   consistent number of samples and matching sample names between datasets,
    -   verify that the sample order is identical across the modalities,
    -   correct formatting of feature names, complete number of features for each dataset,
    -   complete and correctly formatted metadata associated to each dataset
3.  (optional) Depending on 2., identify the nature of missing features, harmonize sample names, correct feature names, harmonize metadata and check for completeness.

```{r processing_harmonize}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| results: false
## Loading the data from the supplemental material from @Gegner2024, the
## sample names are not correctly imported to the *R* session. In this
## step, the sample names are harmonized between the *metabolomics* and the
## *proteomics* dataset. In addition, when checking for the data
## dimension, we observe that the *proteomics* dataset contains
## information from two different extraction methods (number of samples of
## *proteomics*: `r ncol(proteomics)`, number of samples of
## *metabolomics*: `r ncol(metabolomics)`)). We also make sure, that the 
## metadata is harmonized between the two objects and that the 
## information on the tissue type is properly reflected in the metadata. 
## Metadata from Table 1 in @Gegner2024
## is added both to *metabolomics* and *proteomics*.

## data dimension and structure
dim(metabolomics)
dim(proteomics)

## sample names between datasets
##
## metabolomics
## add information on tissue type in column tissue
metabolomics$tissue <- ifelse(metabolomics$Tissue.type == "Tumor tissue", 
    "TU", "NAT")

## harmonize sample names
metabolomics$name <- paste(metabolomics$tissue, 
    rep(1:10, each = 2), sep = "_")
colnames(metabolomics) <- metabolomics$name

## make sure that colData only contains columns name and tissue
colData(metabolomics) <- colData(metabolomics)[, c("name", "tissue")]

## add information on patient_ID
metabolomics$patient_ID <- stringr::str_remove(metabolomics$name, 
    pattern = "NAT_|TU_") |>
    as.numeric()

##
## proteomics
## add information on tissue type in column tissue
proteomics$tissue <- ifelse(grepl(proteomics$name, pattern = "_TU_"), 
    "TU", "NAT")

## make sure that proteomics only contains samples from one extraction method
proteomics <- proteomics[, grep(colnames(proteomics), pattern = "ProtMet")]

## harmonize sample names
proteomics$name <- colnames(proteomics) |>
    stringr::str_remove(pattern = "LFQ.intensity.._KL_") |>
    stringr::str_remove(pattern = "ProtMet_IO_40min_DDA_") |>
    strsplit(split = "_") |>
    lapply(FUN = function(i) i[1]) |>
    unlist() |>
    stringr::str_remove(pattern = "l$")
df_names <- data.frame(
    rbind(
        c("1", "8JDLQY"),
        c("2", "K6R512"),
        c("3", "ZQ021J"),
        c("4", "KWF2HW"),
        c("5", "1FF2F9"),
        c("6", "IJ17TV"),
        c("7", "8U04FR"),
        c("8", "ZT9UTK"),
        c("9", "C5FQXS"),
        c("10", "36AT2O")
    ))
colnames(df_names) <- c("names_table", "names_proteomics")
proteomics$name <- paste(proteomics$tissue, 
    df_names[match(proteomics$name, df_names[["names_proteomics"]]), "names_table"],
    sep = "_")
colnames(proteomics) <- proteomics$name

## make sure that colData only contains columns name and tissue
colData(proteomics) <- colData(proteomics)[, c("name", "tissue")]

## add information on patient_ID
proteomics$patient_ID <- stringr::str_remove(proteomics$name, 
    pattern = "NAT_|TU_") |>
    as.numeric()

## make sure that metabolomics and proteomics have the same colnames
if (!all(colnames(metabolomics) %in% colnames(proteomics)))
    stop("Not all colnames(metabolomics) are found in colnames(proteomics)).")
if (!all(colnames(proteomics) %in% colnames(metabolomics)))
    stop("Not all colnames(proteomics) are found in colnames(metabolomics)).")

## order the colnames according to metabolomics
proteomics <- proteomics[, colnames(metabolomics)]
```

4.  (optional) Identify additional metadata, for example clinical parameters or translated IDs for metabolites (e.g. HMDB, KEGG, ChEBI ids) and proteins (e.g. Entrez, SYMBOL, Uniprot ids), from other sources. Add the metadata to the respective slots of the *SummarizedExperiment* object: *rowData* for feature-related and *colData* 
for sample-related metadata).

```{r processing_add_metadata}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| results: false
## For this dataset, additional metadata on clinical parameters are
## missing. We add information from Table 1 from @Gegner2024 to the
## *SummarizedExperiment* objects. For brievity, the steps can be followed
## in the corresponding GitHub repository of this chapter.

## metadata on patients (Table 1, from Gegner et al., 2024)
metadata <- data.frame(
    rbind(
        c("1", "61", "m", "ADC", "IB",  "0", "Ex- smoker",     "35", "yes"),
        c("2", "80", "f", "ADC", "IIB", "0", "Never- smoker",  "0",  "yes"),
        c("3", "62", "m", "ADC", "IB",  "1", "Ex- smoker",     "40", "yes"),
        c("4", "83", "f", "ADC", "IIB", "0", "Never- smoker",  "0",  "yes"),
        c("5", "56", "f", "ADC", "IIA", "0", "Current smoker", "30", "yes"),
        c("6", "77", "m", "ADC", "IIB", "0", "Ex- smoker",     "10", "no"), 
        c("7", "80", "m", "ADC", "IIB", "0", "Ex- smoker",     "2",  "no"),
        c("8", "72", "f", "ADC", "IIB", "0", "Current smoker", "50", "no"),
        c("9", "60", "m", "ADC", "IB",  "1", "Current smoker", "40", "no"),
        c("10", "57", "m", "ADC", "IB",  "0", "Ex- smoker",     "30", "no")
))
colnames(metadata) <- c("patient_ID", "age_at_diagnosis", "sex", "histology",
    "pstage", "ECOG", "smoking_status", "pack_years", "recurrence")
metadata$patient_ID <- as.numeric(metadata$patient_ID)
metadata$age_at_diagnosis <- as.numeric(metadata$age_at_diagnosis)
metadata$sex <- as.factor(metadata$sex)
metadata$histology <- as.factor(metadata$histology)
metadata$pstage <- as.factor(metadata$pstage)
metadata$ECOG <- as.factor(metadata$ECOG)
metadata$smoking_status <- as.factor(metadata$smoking_status)
metadata$pack_years <- as.numeric(metadata$pack_years)
metadata$recurrence <- as.factor(metadata$recurrence)

## link metadata to colData of metabolomics and proteomics
colData(metabolomics) <- colData(metabolomics) |>
    as.data.frame() |>
    dplyr::left_join(y = metadata, by = "patient_ID") |>
    DataFrame(row.names = colData(metabolomics)$name)
colData(metabolomics) <- colData(proteomics) |>
    as.data.frame() |>
    dplyr::left_join(y = metadata, by = "patient_ID") |>
    DataFrame(row.names = colData(proteomics)$name)

## add Symbol names to rowData of proteomics
uniprots <- Rkeys(org.Hs.egUNIPROT)
dict <- AnnotationDbi::select(org.Hs.eg.db, uniprots, "SYMBOL", "UNIPROT")
uniprot <- dict$UNIPROT
symbol <- dict$SYMBOL
names(symbol) <- uniprot
rowData(proteomics)$SYMBOL <- unlist(lapply(rowData(proteomics)$feature, 
    function(x) paste(symbol[strsplit(x, split = ";")[[1]]], collapse = ";")))
```

### Filtering of features in the datasets

Each dataset includes typically information on the reliability of features, which can be used to remove non-reliable features. This step improves the quality of the datasets. Filtering and removal of the features should be tailored to the study design and the characteristics of the dataset. While stricter filtering can reduce false positives, it may also remove true low-abundance features.

1.  Remove low or high abundance features. Low intensity features may be due to noise or below the limit of detection or quantification. High intensity may be above the limit of quantification, the linear dynamic range, or the calibration range. Removing these features prevents artifacts in downstream statistical analyses. For the targeted metabolomics dataset, ensure that the dataset only includes high-quality metabolite measurements. The MetIDQ output provides assessment of the quality of each measurement, encoded by a color code of each cell (dark blue: \< LOD (Limit of Detection; 3x signal to noise); light blue: \< LLOQ (Lower Limit of Quantification; 10x signal to noise) or \> ULOQ (Upper Limit of Quantification); green: valid; Yellow: Internal Standard out of range). Remove the low-quality features that do not meet the defined quantification criteria (*choice of user*). In this case, we will retain only those metabolites where at least 50% of the measurements fall within the quantification limits (valid). This filtering step improves data reliability and minimizing the impact of imprecise data points.

```{r processing_qc_metidq}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| results: false
source("utils.R")
valid_features <- metIDQ_get_high_quality_features(
    file = "../data/12014_2024_9501_MOESM4_ESM.xlsx", threshold = 0.5)
metabolomics <- metabolomics[valid_features, ]
```

2. Verify that all measurements fall within expected ranges (e.g., non-negative values for intensities). For measurements that fall outside the expected range, correct them, e.g., by setting these values to **NA** (missing data).

```{r}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| results: false
## metabolomics
assay(metabolomics)[which(assay(metabolomics) < 0)] <- NA

## proteomics
assay(proteomics)[which(assay(proteomics) < 0)] <- NA
```

3.  (optional) Remove those features that are only detected in a subset of samples (*choice of user*). High levels of missing data (**NA**) values can reduce statistical power. While imputation techniques can be applied, imputation of excessive numbers of missing values per feature can lead to unreliable results. Additionally, features identified in only a few biological replicates may lack reproducibility, impacting downstream analyses. Filtering criteria can be set on a global level, such as removing features absent in more than 50% of the samples, or based on metadata, such as removing features detected in fewer than two biological replicates per condition. More stringent thresholds, such as requiring detection in at least 50% of replicates within each condition, can further improve dataset reliability. The choice of filtering thresholds should be guided by domain-specific knowledge and the characteristics of the datasets. Overly strict filtering may remove biologically relevant proteins that are present only in specific subsets of samples, such as particular cell lines or patient groups. Filtering strategies should be carefully considered to maintain a balance between data quality and the retention of meaningful biological information.

```{r processing_qc_proteomics_remove_low_number_features}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| results: false
## we will remove the features that are detected in less than ten samples 
## metabolomics
valid_features <- apply(assay(metabolomics), MARGIN = 1, FUN = function(rows_i) !(sum(!is.na(rows_i)) < 10))
metabolomics <- metabolomics[valid_features, ]

## proteomics
valid_features <- apply(assay(proteomics), MARGIN = 1, FUN = function(rows_i) !(sum(!is.na(rows_i)) < 10))
proteomics <- proteomics[valid_features, ]
```

4.  Remove the features with high coefficient of variation (CV, *choice of user*). High variability in technical replicates or QC pools suggests inaccurate quantification. For example, features with CV \> 30-50% of raw intensities across technical replicates may be removed.

5. (optional) Remove features with a standard deviation of 0 as these features do not vary across samples and do not contain information for biological interpretation.

```{r processing_qc_sd}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| results: false
## metabolomics
valid_features <- apply(assay(metabolomics), MARGIN = 1, sd, na.rm = TRUE) > 0
metabolomics <- metabolomics[valid_features, ]

## proteomics
valid_features <- apply(assay(proteomics), MARGIN = 1, sd, na.rm = TRUE) > 0
proteomics <- proteomics[valid_features, ]
```

6.  For proteomics data, ensure that the dataset only includes high-quality protein measurements. In the proteomics dataset generated by MaxQuant [@Cox2008], peptide counts serve as an indicator of the reliability of protein identification. To ensure data quality, we will filter out proteins identified with low confidence (*choice of user*). In this case, we will remove any protein that has fewer than two peptides supporting its identification. This approach helps retain only well-supported protein identifications, enhancing the robustness of downstream analyses.

```{r processing_qc_peptide_counts}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| results: false
valid_features <- lapply(
    strsplit(rowData(proteomics)$peptide_counts_all, split = ";"), 
    function(i) max(as.numeric(i)) >= 2) |> 
    unlist()
valid_features[is.na(valid_features)] <- FALSE
proteomics <- proteomics[valid_features, ]
```

7.  For proteomics data, remove common contaminants by using a contaminant database (e.g., MaxQuant's contaminant list). Contaminants like keratins (from human skin), trypsin (from digestion), or bovine serum albumin (BSA) can interfere with interpretation of results.

8.  For the proteomics dataset, remove the features with reverse sequences as they do not correspond to real biological proteins. Reverse sequences refer to decoy sequences generated by reversing the amino acid sequence of real proteins. Reverse sequences are commonly used as negative controls in target-decoy strategies for false discovery rate estimation in mass-spectrometry-based proteomics.

```{r processing_qc_reverse}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| results: false
valid_features <- rowData(proteomics)$reverse != "+" 
valid_features[is.na(valid_features)] <- TRUE
proteomics <- proteomics[valid_features, ]
```

### Quality assessment and quality control (QA/QC)

The next step in the processing pipeline is to assess the quality of the data, including both the measurements and the metadata values, and to control for it.

The *MatrixQCvis* package [@Naake2022] performs QA/QC on *SummarizedExperiment* objects and offers users to perform these steps in an interactive *shiny* application.

1.  Launch the application via *shinyQC(metabolomics)* or *shinyQC(proteomics)* to begin the quality assessmet for the respective datasets.
2.  Check the number of measured and missing values per sample and per feature. The metric informs about the dynamic range of the acquisition. Differences between samples of an experiment may indicate differences in the dynamic range and/or in the sample content.
3.  Check the distribution of measurements per sample. The metric informs about the dynamic range of the acquisition. Differences between samples of an experiment may indicate differences in the dynamic range and/or in the sample content.
4.  Examine the mean-sd plot (Figure 1 A and B). The metric informs about the level of homoskedasticity of the dataset which may be important for parametric tests. Homoscedasticity is an assumption of parametric tests, e.g. t-tests. In case of sd-mean independence, the running median should be approximately horizontal.
5.  Examine the MA plots and Hoeffding's D statistic plot. The metric informs about systematic biases and variability in the data by taking into account the log2-fold change between two conditions (*M = log2(I_i) - log2(I_j)*) and the mean expression of two conditions (*A = 1/2 (log2(I_i) + log2(I_j))*), where *I_i* can be the intensity from one sample i and *I_j* the averaged intensities from a set of samples excluding sample *i*. Hoeffding's *D* statistic measures the dependency between *A* and *M*. It is a measure of the distance between *F(A, M)* and *G(A)H(M)*, where *F(A, M)* is the joint cumulative distribution function (CDF) of *A* and *M*, and *G* and *H* are marginal CDFs. The higher the value of *D*, the more dependent are *A* and *M*.
6.  Examine the empirical cumulative distribution function (ECDF) plots. The metric informs about the distribution, the skewness of a sample and helps to identify outliers.
7.  Examine the distance matrix between samples (Figure 1 C and D). The metric informs how similar or different samples are from one another based on the measured values. It informs about relationships and consistency between samples of the same group (e.g. treatment vs. control). The metric also helps to detect outliers and to identify batch effects.
8.  Examine dimension reduction, e.g. principal component analysis (PCA, Figure 1 E and F) and loadings plot. PCA preserves the variance between data points and aids in exploring relationships between samples and features. PCA is helpful in identifying structure in the dataset and in detecting outliers.
9.  Identify and remove any outliers based on the steps above. Reassess the data after the removal by repeating the steps 1.-8.
10. After reviewing the metrics 3.-9., identify the most appropriate methods for normalization, batch correction, transformation, and imputation for your datasets to ensure consistency and quality across samples and features.

```{r processing_qc_matrixqcvis_prepare_plots, fig.show="hide"}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| results: false
## A: meanSdPlot of raw values
gg_meanSd <- meanSdPlot(assay(proteomics), ranks = TRUE)$gg +
    theme(legend.position = "none")

## B: meanSdPlot of transformed values
gg_meanSd_log <- meanSdPlot(log(assay(proteomics)), ranks = TRUE)$gg + 
    theme(legend.position = "none")

## C: PCA of imputed raw values
pca <- assay(proteomics) |>
    imputeAssay(method = "MinDet") |>
    dimensionReduction(type = "PCA", 
        params = list(center = TRUE, scale = TRUE))
explainedVar <- explVar(pca[[2]], type = "PCA")
gg_pca <- dimensionReductionPlot(tbl = pca[[1]], se = proteomics, 
    color = "tissue", x_coord = "PC1", y_coord = "PC2", interactive = FALSE,
    explainedVar = explainedVar) +
    guides(color = guide_legend(title = "tissue"))

## D: PCA of imputed raw values
pca <- assay(proteomics) |>
    transformAssay(method = "log") |>
    imputeAssay(method = "MinDet") |>
    dimensionReduction(type = "PCA", 
        params = list(center = TRUE, scale = TRUE))
explainedVar <- explVar(pca[[2]], type = "PCA")
gg_pca_log <- dimensionReductionPlot(tbl = pca[[1]], se = proteomics, 
    color = "tissue", x_coord = "PC1", y_coord = "PC2", interactive = FALSE,
    explainedVar = explainedVar) +
    guides(color = guide_legend(title = "tissue"))

## E: distance matrix of raw values
d <- assay(proteomics) |>
    distShiny()
gg_dist <- distSample(d, proteomics, label = "tissue", 
    show_row_names = FALSE, title = "") |>
    as.ggplot()

## F: distance matrix of transformed values
d <- assay(proteomics) |>
    transformAssay(method = "log") |>
    distShiny()
gg_dist_log <- distSample(d, proteomics, label = "tissue", 
    show_row_names = FALSE, title = "") |>
    as.ggplot()
```

```{r figure_1}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| results: false
g <- plot_grid(gg_meanSd, gg_meanSd_log, gg_dist, gg_dist_log, 
    gg_pca, gg_pca_log, ncol = 2, labels = "AUTO")
ggsave(filename = "figure/figure1.jpg", plot = g, device = "jpg", dpi = 600)
```

### Normalization, batch correction, transformation, missing value imputation

Tools, such as MatrixQCvis [@Naake2022] can be used to explore data quality interactively and to optimize the choice of batch correction, normalization, or transformation methods. The *MatrixQCvis* package offers utility functions to perform normalization, batch reduction, transformation and imputation using a variety of methods (*normalizeAssay*, *batchCorrectionAssay*, *transformAssay*, *imputeAssay*).

1.  (optional) Normalize the dataset. Normalization adjusts for technical variability and ensures comparability of metabolite and protein levels across samples. The choice of method depends on the dataset’s variability and experimental design. Common methods are median normalization, quantile division normalization (division of measurements of each sample by the given quantile, e.g. 50% for median division), sum normalization (division of measurements of each sample by the sum of intensities column for that sample).
2.  (optional) Correct for batches. Batch effect correction is typically essential in multi-batch experiments acquired over a longer time, but may be not needed in smaller scale experiments. *ComBat*, for instance, requires specifying batch labels and optionally including covariates to retain biological variability. In case of linear effects of batches the batch effect can be removed via *limma*'s *removeBatchEffect* function and specifying the batch labels.
3.  Transform the dataset. For example, *log* transformation is commonly used to reduce skewness and heteroskedasticity, other methods, such as *log2*, *log10*, or variance stabilizing normalization (vsn) can be applied as well. After transformation, check the mean-sd plot to assess homoskedasticity and determine if the transformation has balanced the variance.
4.  (optional) Scale the features. Scaling standardizes the magnitude of intensities across features, making them comparable by putting them on a common scale (e.g., z-score).
5.  (optional) Impute the missing values (**NA**) of the datasets. Metabolomics and proteomics data acquired by mass spectrometry typically contain missing values. The choice of the imputation method depends on the mechanism of missingness (e.g., missing completely at random, missing at random, missing not at random). Various imputation techniques are available in *MatrixQCvis*, such as k-nearest neighbors imputation or imputation by minimum values. For a more in-depth discussion on imputation methods in metabolomics, refer to @Wei2018, and for proteomics, see @Lazar2016. Generally, missing values should be imputed with caution, as doing so requires assuming a model of missingness and can introduce bias. In many cases, downstream analyses can tolerate a certain level of missing values without significant impact on results. Tools such as *limma* and *MOFA* are designed to handle datasets with incomplete observations, making imputation unnecessary for many applications.

```{r processing_normalization}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| results: false
## In the case of the lung cancer adenocarcinoma dataset,
## we will apply quantile division normalization and log transformation to control
## for heteroskedasticity

## metabolomics
assay(metabolomics) <- assay(metabolomics) |>
    normalizeAssay(method = "quantile division", probs = 0.5,
        multiplyByNormalizationValue = TRUE) |>
    transformAssay(method = "log")

## proteomics
assay(proteomics) <- assay(proteomics) |>
    #normalizeAssay(method = "quantile division", probs = 0.5, 
    #    multiplyByNormalizationValue = TRUE) |>
    transformAssay(method = "log")
```

## Multi-omics integration using MOFA

MOFA [@Argelaguet2018] integrates omics modalities using a factor analysis to uncover latent factors that explain the shared variation across different datasets. Proper data wrangling and processing as discussed in previous sections (e.g., normalization, transformation) are critical steps in ensuring the reliability and interpretability of the results.

### Model building

1.  Define the modalities (omics data types) to be included in the model. Make sure that the modalities are properly normalized and transformed. (optional) Define group information. Group information refers to the different subsets or categories within the data that can be used to model variation across multiple omics layers. This could represent different experimental conditions, sample types (e.g., tumor vs. non-tumor), or other biological categories that might drive the variation in the data.

```{r mofa_create_mofa}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| results: false
# prepare data for MOFA
MOFAobject <- create_mofa(
    list(
        metabolomics = assay(metabolomics),
        proteomics = assay(proteomics)
))
```

2.  Define the parameters for training of the MOFA model (*choice of user*). Parameters control how data is preprocessed before training, such as whether views or groups are scaled to the same variance or centered (with a mean of zero). Parameters can define the structure of the MOFA model, such as the number of latent factors to infer, which can be guided by prior biological knowledge or model selection techniques. The user can also specify if a spike-and-slab prior on factor loadings is applied, which promotes sparsity in factor loadings by ensuring that only a few features have strong contributions while others are near zero. The user may apply the automatic relevance determination (ARD) priors on factor loadings, allowing the model to automatically determine which factors are relevant for each view. In addition, parameters can specify the training process of the model, including the mode of convergence of the algorithm, the use of stachastic inference, the seed for reproducibility, and the maximum number of iterations before the model training is interrupted.

3.  Add the model parameters to the untrained MOFA model and initiate the training of the model. The model will learn latent factors that explain the shared variation across the different omics datasets, based on the specified configurations and data characteristics.

```{r mofa_run_mofa}
#| eval: true
#| echo: false
#| message: false
#| warning: false 
#| results: false
## define the model parameterss
## data options
data_opts <- get_default_data_options(MOFAobject)
data_opts$scale_views <- FALSE ## default
data_opts$scale_groups <- FALSE ## default
data_opts$center_groups <- TRUE ## default

## model_options
model_opts <- get_default_model_options(MOFAobject)
model_opts$num_factors <- 5
model_opts$spikeslab_weights <- TRUE ## default
model_opts$ard_weights <- TRUE ## default

## training options
train_opts <- get_default_training_options(MOFAobject)
train_opts$convergence_mode <- "slow"
train_opts$stochastic <- FALSE
train_opts$seed <- 2025
train_opts$maxiter <- 10000

## prepara MOFA and run
MOFAobject <- prepare_mofa(
    object = MOFAobject,
    data_options = data_opts,
    model_options = model_opts,
    training_options = train_opts
)

## initialize MOFA model
#reticulate::py_install("mofapy2")
MOFAobject <- run_mofa(MOFAobject, 
    use_basilisk = FALSE)
```

4.  Inspect the sanity of the model output. Plot data overview and verify that the dimensions for each dataset are as expected. Calculate the correlation scores between the factors (Figure 2 A). The correlation coefficients between the factors should be low, indicating that each factor captures distinct variation in the data. If factors are found to be highly correlated, it may suggest that the model is not effectively separating the variation across the datasets. In such cases, consider reducing the number of trained factors. This will not only help in improving the interpretability of the model but also enhance the biological relevance of the factors by focusing on more distinct sources of variation.

```{r mofa_data_overview}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| results: false
#| fig-show: hide
## plot data overview
plot_data_overview(MOFAobject)
```

```{r mofa_correlation_between_factors}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| results: false
## sanity check: correlation between factors
png("figure/figure2_cor.png", width = 1500, height = 1500, res = 300)
plot_factor_cor(object = MOFAobject, method = "pearson")
dev.off()

#img <- png::readPNG("figure/figure2_tmp.png")
#g <- grid::rasterGrob(img, interpolate = TRUE)

#gg_cor <- ggplot() +
#  annotation_custom(g, xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = Inf) +
#  theme_void()
```

5.  Examine explained variance of the factors and per modality (Figure 2 B). The analysis shows the proportion of variance explained by each latent factor across different views (modalities) and groups (if applicable). Higher variance explained per factor means that the factor captures a significant portion of the variation in that particular view. If the first few factors explain most of the variance, it suggests that only a subset of factors drive the structure in the data. If certain factors explain more variance in one view compared to another, it suggests that the factor is more specific to that modality. Co-variance in most factors indicates a strong relationship between the data types. If plotted separately, differences in variance explained between groups may indicate distinct biological patterns or variation in data structure across conditions.

```{r mofa_variance_explained}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| results: false
#| fig-show: hide
## plot explained variance
png("figure/figure2_variance.png", width = 1500, height = 1500, res = 300)
plot_variance_explained(MOFAobject, x = "view", y = "factor", 
    plot_total = FALSE)## |>
    ##as.ggplot()
dev.off()

## plot variance explained per data modality.
plot_variance_explained(MOFAobject, x = "view", y = "factor",
    plot_total = TRUE)[[2]]

## Alternatively: Create a plot where the "activity" is shown.
## The activity is defined by a contribution of e.g. \>=30% variance explained
## in each layer per data modality, e.g. if a modality explains 28%
## variance of Factor1, then it will be set to inactive.
## plot active/inactive per data modality
.var <- calculate_variance_explained(MOFAobject)$r2_per_factor$group1
apply(.var, MARGIN = 1, 
        function(row_i) 100* row_i / sum(row_i)) |>
    apply(MARGIN = 2, 
        FUN = function(column_i) ifelse(column_i >= 30, "active", "inactive")) |>
    as.data.frame() |>
    tibble::rownames_to_column(var = "dataset") |>
    tidyr::pivot_longer(cols = starts_with("F"), names_to = "Factor") |>
    dplyr::mutate(dataset = factor(dataset),
        value = factor(value), 
        Factor = factor(Factor, levels = rownames(.var))) |>
    ggplot(aes(x = dataset, y = Factor, fill = value)) +
        geom_tile(stat = "identity", color = "black") +
        scale_fill_manual(values = c("active" = "#505050", "inactive" = "#eeeeee")) +
        xlab("") + ylab("")
```

### Model interpretation: Variance decomposition and analysis of factors

1.  Obtain sample covariates and cofactors and add to metadata. Metadata can
be retrieved from the *colData* slot of the *SummarizedExperiment* objects.

```{r mofa_add_covariates}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| results: false
## define covariates and add to metadata
covariates <- colData(metabolomics) |>
    as.data.frame() |>
    dplyr::select(-c(histology))
MOFAobject@samples_metadata <- dplyr::left_join(
    MOFAobject@samples_metadata, 
    covariates, by = c("sample" = "name"))
```

2.  Analyse the association between latent factors and metadata (e.g., clinical metadata, information on demographics, experimental condition, Figure 2 C). Compute correlations between the learned MOFA factors and the user-specific covariates (continous or categorical). Visualize and examine the correlation coefficients and p-values. The intensity and sign of the color indicate the strength and direction of the correlation. Positive correlations suggest that an increase in the covariate is associated with an increase in the factor values. Negative correlations suggest an inverse relationship.

```{r mofa_correlate_factors}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| results: false
#| fig-show: hide
## correlation with covariates
png("figure/figure2_cor_metadata.png", width = 1500, height = 1500, res = 300)
correlate_factors_with_covariates(MOFAobject, 
    covariates = c("tissue", "patient_ID", "age_at_diagnosis",
        "sex", "pstage", "ECOG", "smoking_status", "pack_years",
        "recurrence"), 
    plot = "r")
dev.off()
#img <- png::readPNG("figure/figure2_tmp.png")
#g <- grid::rasterGrob(img, interpolate = TRUE)

#gg_cor_metadata <- ggplot() +
#  annotation_custom(g, xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = Inf) +
#  theme_void()

## p-values of correlation with covariates
correlate_factors_with_covariates(MOFAobject, 
    covariates = c("tissue", "patient_ID", "age_at_diagnosis",
        "sex", "pstage", "ECOG", "smoking_status", "pack_years",
        "recurrence"), 
    plot = "log_pval", alpha = 0.05)
```

3.  Plot the separation of the samples by factors (Figure 2 D). Each factor arranges the samples along a one-dimensional axis centered at zero, where the absolute value is not important; only the relative positioning of the samples matters. Samples with different signs on the axis indicate opposite "effects" along the inferred variation axis, with larger absolute values suggesting stronger effects. The interpretation of these factors is similar to that of principal components in PCA, where each factor represents a direction of variation in the data, and the separation of samples along these factors provides insight into the underlying patterns.

```{r mofa_separation_samples_factors}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| results: false
#| fig-show: hide
allFactors <- get_factors(MOFAobject, factors = "all", as.data.frame = TRUE) |>
    tibble::as_tibble() |>
    dplyr::mutate(factor = gsub("Factor","F", factor))
allFactors <- dplyr::left_join(allFactors, covariates, by = c("sample" = "name"))
plotTab <- filter(allFactors, factor %in% c("F1", "F2", "F3", "F4", "F5")) |>
    tidyr::spread(key = factor, value = value)
plotTab$tissue <- as.character(plotTab$tissue)
plotTab$patient_ID <- as.factor(plotTab$patient_ID)
plotTab$sex <- as.factor(plotTab$sex)

## Factor 1 and 2
gg_factor1_factor2 <- ggplot(plotTab, aes(x = F1, y = F2)) +
    geom_point(aes(x = F1, y = F2, color = patient_ID, shape = tissue),
        stroke = 2, size = 4) +
    scale_shape_manual(values = c(TU = 21, NAT = 22)) +
    guides(color = guide_legend(ncol = 2)) +
    ggnewscale::new_scale_colour() +
    geom_point(aes(color = sex), size = 2) +
    guides(color = guide_legend(ncol = 2)) +
    xlab("Factor 1") + ylab("Factor 2") +
    theme_bw()

png("figure/figure2_factor1_factor2_simplified.png", width = 1500, height = 1500, res = 300)
ggplot(plotTab, aes(x = F1, y = F2)) +
    geom_point(aes(x = F1, y = F2, color = tissue),
        stroke = 2, size = 4) +
    scale_color_manual(values = c(NAT = "#1F78B4", TU = "#FF7F00")) +
    ggnewscale::new_scale_colour() +
    geom_point(aes(color = sex), size = 2) +
    scale_color_manual(values = c(f = "#33A02C", m = "#6A3D9A")) +
    xlab("Factor 1") + ylab("Factor 2") +
    theme_bw()
dev.off()

## Factor 1 and 3
gg_factor1_factor3 <- ggplot(plotTab, aes(x = F1, y = F3)) +
    geom_point(aes(color = patient_ID, shape = tissue), stroke = 2, size = 4) +
    scale_shape_manual(values = c(TU = 21, NAT = 22)) +
    guides(color = guide_legend(ncol = 2)) +
    ggnewscale::new_scale_colour() +
    geom_point(aes(color = sex), size = 2) +
    guides(color = guide_legend(ncol = 2)) +
    xlab("Factor 1") + ylab("Factor 3") +
    theme_bw()
```

4.  Analyse the weights of the factors (Figure 2 E). The weights represent how strongly each feature is associated with each factor. Features with little or no association to a factor will have values near zero, while those with a strong association will exhibit large absolute values. The sign of the weight indicates the direction of the effect: a positive weight means the feature has higher lievels in samples with positive factor values, while a negative weight indicates higher levels in samples with negative factor values.

```{r mofa_plot_top_weights}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| results: false
#| fig-show: hide

png("figure/figure2_top_weights.png", width = 1500, height = 1500, res = 300)
plot_top_weights(MOFAobject,
  view = "metabolomics",
  factor = 1,
  nfeatures = 10, ## number of features to highlight
  scale = TRUE,   ## scale weights from -1 to 1
  abs = FALSE     ## take the absolute value?
)
dev.off()
```

5.  Analyse the feature intensities across samples and relationships between features using the information from the MOFA model (Figure 2 F). The visualizations are helpful for identifying patterns clusters, assessing data quality and batch effects, detecting potential outliers, and understanding how different features, factors, or sample groups contribute to the overall variation in the dataset. The visualizations should be created for each factor of interest and for the most important (e.g. top 10 or 20) features per factor (*choice of user*).

```{r mofa_plot_data_heatmap}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| results: false
#| fig-show: hide
png("figure/figure2_heatmap.png", width = 1500, height = 1500, res = 300)
plot_data_heatmap(MOFAobject, view = "metabolomics", 
    factor = 1, features = 10,
    ## extra arguments that are passed to the `pheatmap` function
    cluster_rows = TRUE, cluster_cols = FALSE, show_rownames = TRUE, 
    show_colnames = FALSE, annotation_samples = "tissue")
dev.off()
 
plot_data_scatter(MOFAobject, view = "metabolomics", factor = 1, features = 6, 
    add_lm = TRUE, color_by = "tissue")
```


```{r figure_2}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| results: false
#| fig-show: hide
## load images
gg_cor <- image_read("figure/figure2_cor.png")
gg_variance <- image_read("figure/figure2_variance.png")
gg_cor_metadata <- image_read("figure/figure2_cor_metadata.png") |>
    image_crop(geometry = "1500x1250+0+200")
gg_factor1_factor2_simplified <- image_read("figure/figure2_factor1_factor2_simplified.png")
gg_top_weights <- image_read("figure/figure2_top_weights.png")
gg_heatmap <- image_read("figure/figure2_heatmap.png")

## convert images to ggplot objects
gg_cor <- ggdraw() + draw_image(gg_cor)
gg_variance <- ggdraw() + draw_image(gg_variance)
gg_cor_metadata <- ggdraw() + draw_image(gg_cor_metadata)
gg_factor1_factor2_simplified <- ggdraw() + draw_image(gg_factor1_factor2_simplified)
gg_top_weights <- ggdraw() + draw_image(gg_top_weights)
gg_heatmap <- ggdraw() + draw_image(gg_heatmap)

g <- plot_grid(gg_cor, gg_variance, 
                gg_cor_metadata, gg_factor1_factor2_simplified, 
                gg_top_weights, gg_heatmap,
     ncol = 2, rel_widths = c(1, 1), labels = "AUTO")
ggsave(filename = "figure/figure2.jpg", plot = g, device = "jpg", dpi = 600)
```


## Differential Expression Analysis

Differential analysis identifies metabolites with significant changes between conditions. Selecting appropriate statistical tests depends on the data distribution and experimental design.

### Statistical Testing
To systematically define which statistical tests to consider one can follow these steps:

1. Define which comparison(s) should be performed, which depends on the metainformation in your data and on the biological question. For ***one_vs_one (single comparison)*** analysis, where a single condition is compared against another (e.g. numerator= “Condition1” and denominator = “Condition2”), statistical tests such as the Student’s t-test, Wilcoxon rank-sum test, Chi-squared test, correlation test, or linear regression modeling can be used. For ***all_vs_one (multiple comparison against a reference)***, where multiple conditions are compared individually against a single reference condition (e.g., numerator = ["Condition2", "Condition3", ..., "ConditionN"], denominator = "Condition1") or ***all_vs_all (multiple comparison)*** , where all conditions are compared against each other (e.g., numerator = ["Condition1", "Condition2", ..., "ConditionN"], denominator = ["Condition1", "Condition2", ..., "ConditionN"]), multiple testing has to be performed for which statistical tests such as ANOVA (analysis of variance), Welch’s ANOVA, Kruskal-Wallis test, or linear modeling (e.g., using limma’s linear model fitting) are appropriate.

2. Check assumption of normality to understand if the data follow a normal- or not-normal distribution by for example performing a Shapiro test. For normally distributed data, t-tests or ANOVA can be used, whilst for non-normally distributed data, non-parametric tests like Wilcoxon rank-sum test or Kruskal-Wallis test are appropriate.

3. Check if the data are homoscedastic (equal variance) using Levene's test or Bartlett's test. If the data are not homoscedastic, consider using Welch’s ANOVA or Wilcoxon rank-sum test.

4. Are there missing values (NAs) in the data? If this is the case tests such as linear modeling can to be used, auch as limma, which fits a linear model to the data and can handle missing values. Yet, the limma method assumes that the residuals of the linear model are approximately normally distributed.

Here we use the R package [MetaProViz](https://saezlab.github.io/MetaProViz/index.html) to perform differential analysis since it automatically performs both, the shapiro.test for normality and the Bartlett's test for homoscedasticity, and the choice between different statistical test [@Schmidt2025]. Importantly, MetaProViz also adjusts for multiple testing using Benjamini Hochberg, False Discovery Rate (FDR), Bonferroni or Holm [@Schmidt2025].

For simplicity, we will showcase an example performing a one-versus-one comparison of tumour versus normal.
```{r}
#| eval: true
#| echo: false
#| message: true
#| warning: false
#| results: false

InputData <- as.data.frame(t(SummarizedExperiment::assay(metabolomics)))%>% 
  dplyr::mutate(across(everything(), ~ tidyr::replace_na(.x, 0)))

# Differential metabolite analysis
DMA_Res <- MetaProViz::DMA(InputData= InputData,
                           SettingsFile_Sample= as.data.frame(SummarizedExperiment::colData(metabolomics)),
                           SettingsInfo = c(Conditions="tissue", Numerator="TU" , Denominator = "NAT"),
                           SettingsFile_Metab = as.data.frame(SummarizedExperiment::rowData(metabolomics)),
                           StatPval ="t.test",
                           StatPadj="fdr",
                           PerformShapiro = TRUE,
                           PerformBartlett = TRUE,
                           SaveAs_Plot = NULL,
                           SaveAs_Table = NULL)
```

The Shapiro test results shows that the majority of the metabolites follow mostly normal distribution, which means it is appropriate to perform a t-test (Fig.3a-b). Yet, it is important to note that almost half of the metabolites are not normally distributed, which means that the t-test may not be appropriate for these metabolites. In this case, it might be better to use a non-parametric test such as the Wilcoxon rank-sum test or Kruskal-Wallis test. 

```{r}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| results: false

DMA_Res[["ShapiroTest"]][["DF"]]%>%
  as.data.frame()%>%
  dplyr::select(1:5)%>%
  kableExtra::kbl(caption = "`Shaprio.test` results.") %>%
  kableExtra::kable_classic(full_width = F, html_font = "Cambria", font_size = 12) 


Fig3A <- DMA_Res[["ShapiroTest"]][["Plot"]][["Distributions"]][["NAT"]] + 
  #ggplot2::ggtitle("Normal distribution of NAT samples") +
  ggplot2::labs(subtitle = "39.27% of metabolites are not normally distributed based on Shapiro test.")

Fig3B <- DMA_Res[["ShapiroTest"]][["Plot"]][["Distributions"]][["TU"]]+
  ggplot2::labs(subtitle = "46.57% of metabolites are not normally distributed based on Shapiro test.")


```
```{r}
#| eval: true
#| echo: false
#| message: true
#| warning: false
#| results: false

InputData <- as.data.frame(t(SummarizedExperiment::assay(metabolomics)))%>% 
  dplyr::mutate(across(everything(), ~ tidyr::replace_na(.x, 0)))

# Differential metabolite analysis
DMA_Res_wilcox <- MetaProViz::DMA(InputData= InputData,
                                  SettingsFile_Sample= as.data.frame(SummarizedExperiment::colData(metabolomics)),
                                  SettingsInfo = c(Conditions="tissue", Numerator="TU" , Denominator = "NAT"),
                                  SettingsFile_Metab = as.data.frame(SummarizedExperiment::rowData(metabolomics)),
                                  StatPval ="wilcox.test",
                                  StatPadj="fdr",
                                  PerformShapiro = TRUE,
                                  PerformBartlett = TRUE,
                                  SaveAs_Plot = NULL,
                                  SaveAs_Table = NULL)
```

Checking the results of the differential analysis by plotting a Volcano plot, we can see the proportion of the metabolites that are significantly up- or down-regulated between the two conditions for both, the t-test and Wilcoxon rank-sum test (Fig.3c-d). 
```{r}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| results: false

# data table inspection
DMA_Res[["DMA"]][["TU _vs_ NAT"]]%>%
  as.data.frame()%>%
  dplyr::select(1,23:28)%>%
  dplyr::arrange(p.adj)%>%
  dplyr::slice(1:10) %>% 
  kableExtra::kbl(caption = "DMA results Tumour versus Normal.") %>%
  kableExtra::kable_classic(full_width = F, html_font = "Cambria", font_size = 12) 

# Volcano plots
## 1. t.test
InputData <- DMA_Res[["DMA"]][["TU _vs_ NAT"]]%>%
  tibble::column_to_rownames("Metabolite")

Res <- MetaProViz::VizVolcano(InputData=InputData,
                               SelectLab = NULL,
                               ylab ="-log10(p.adj)",
                               yCutoff = 0.1,
                               PlotName = "t-test & FDR adjustment")


Fig3C <- Res[["Plot_Sized"]][["Plot_Sized"]]


InputData <- DMA_Res_wilcox[["DMA"]][["TU _vs_ NAT"]]%>%
  tibble::column_to_rownames("Metabolite")

Res <- MetaProViz::VizVolcano(InputData=InputData,
                               SelectLab = NULL,
                               ylab ="-log10(p.adj)",
                               yCutoff = 0.1,
                               PlotName = "Wilcoxon test & FDR adjustment")


Fig3D <- Res[["Plot_Sized"]][["Plot_Sized"]]

```

### Visualization of differential analysis results

The classical Volcano plot of the differential analysis results(Fig.3b) displays the log2 fold change on the x-axis, while the y-axis represents the negative logarithm of the p-adjusted value. The volcano plot is a scatter plot that displays the relationship between the magnitude of change (log2 fold change) and the significance (p adjusted value) of each metabolite. Points above a certain threshold are considered statistically significant and hence it is important to set thresholds for significance (e.g. p.adj < 0.05) and log2 fold change(e.g. |log2FC| > 0.5).

With [MetaProViz](https://saezlab.github.io/MetaProViz/index.html) visualisation, we can easily add additional relevant information to the plot [@Schmidt2025]. First it can be relevant to add the metabolite name to the plot and colour code for metabolite class, which describes the metabolite class as provided by Biocrates (Fig. 3e). 
```{r}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| results: false

InputData <- DMA_Res_wilcox[["DMA"]][["TU _vs_ NAT"]]%>%
  tibble::column_to_rownames("Metabolite")

SettingsFile_Metab <- DMA_Res_wilcox[["DMA"]][["TU _vs_ NAT"]]%>%
  tibble::column_to_rownames("Metabolite")%>%
  dplyr::select(24:28)

Res <- MetaProViz::VizVolcano(InputData=InputData,
                       SettingsInfo= c(color="class"),
                       SettingsFile_Metab= SettingsFile_Metab,
                       ylab ="-log10(p.adj)",
                       yCutoff = 0.1,
                       PlotName = "Wilcoxon test & FDR adjustment")

Fig3E <- Res[["Plot_Sized"]][["Plot_Sized"]]
```

Displaying all metabolite classes on the volcano plot can make it difficult to interpret (Fig. 3d). For better visibility individual plots for each of the metabolite class can be created (Fig. 3f-h).
```{r}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| results: false

InputData <- DMA_Res_wilcox[["DMA"]][["TU _vs_ NAT"]]%>%
  tibble::column_to_rownames("Metabolite")%>%
  dplyr::filter(class %in% c("Ceramides", "Triacylglycerols", "Sphingolipids"))
                    
SettingsFile_Metab <- DMA_Res_wilcox[["DMA"]][["TU _vs_ NAT"]]%>%
                                            tibble::column_to_rownames("Metabolite")%>%
                                            dplyr::select(24:28)

Res <- MetaProViz::VizVolcano(InputData=InputData ,
                       SettingsInfo= c(individual="class"),
                       SettingsFile_Metab= SettingsFile_Metab,
                       SelectLab = NULL,
                       ylab ="-log10(p.adj)",
                       yCutoff = 0.1,
                       PlotName = "Wilcoxon & FDR")

Fig3F <- Res[["Plot_Sized"]][["Ceramides"]]
Fig3G <- Res[["Plot_Sized"]][["Sphingolipids"]]
Fig3H <- Res[["Plot_Sized"]][["Triacylglycerols"]]  
```

```{r figure_3}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| results: false


row1 <- plot_grid(Fig3A, Fig3B, ncol = 2, labels = c("A", "B"))
row2 <- plot_grid(Fig3C, Fig3D, Fig3E, ncol = 3, labels = c("C", "D", "E"))
row3 <- plot_grid( Fig3F, Fig3G, Fig3H,ncol = 3, labels = c("F", "G", "H"))
g <- plot_grid(row1, row2, row3, ncol = 1, rel_heights = c(0.7, 1, 1))

ggsave(filename = "figure/figure3.jpg", 
       plot = g, 
       device = "jpg", 
       dpi = 600, 
       width = 16, 
       height = 13) 
```

## Analysis using biological prior knowledge
Prior Knowledge (PK) is a powerful tool in omics data analysis, allowing researchers to leverage existing biological information to gain mechanistic insights into their data. Here the type of prior knowledge will dictate the biological question that can be answered and how the results can be interpreted. Indeed, we can divide PK in three main categories:
1. Collections of biological pathways or processes meaning sets of genes/metabolites with common characteristic (e.g. GO-term, KEGG,etc.), which are usable for classical mapping. This is commonly also referred to as pathway analysis and the key principle to show that the differentially expressed features are statistically more enriched in specific pathways and not randomly distributed [@García-Campos_2015]. Here the dysregulation of a pathway is inferred from measurements of its own components (e.g. metabolite abundance).
2. Collections of enzymes/molecules and their targets meaning for example kinases and target phosphopeptides, which are usable for footprint analysis. Footprint analysis estimates the activities from molecular readouts of the targets (e.g. phosphosites) downstream of the enzyme (e.g. kinase) [@Dugourd2019].
3. Collections of enzymes/molecules, their targets and the interaction between them, which will depict a PK network (PKN), for which network-based approaches are needed to systematically integrate the data and extract coherent patterns. 

In case of classical mapping and fooprint analysis, basic statistical methods such as Gene Set Enrichment Analysis (GSEA), Over-Representation Analysis (ORA), MLM, ULM, etc. can be used to obtain a score for the activity of a kinase (=footprinting) or the dysregulation of a pathway (=mapping) [@BadiaIMompel2022]. Yet, it is important to understand that the choice of statistical method will influence the results and the biological interpretation. Indeed, GSEA uses a feature list ranked by their expression changes between conditions and hence finds pathways enriched at the top or bottom of the ranked gene list, whilst ORA is based on the fishers exact test and checks if for example a selection of upregulated features (e.g. Log2FC >1, p.adj < 0.05) are over-represented in a specific pathway. Apart from this biological perspective, also the methods performance should be taken into account. Indeed, in a benchmark the performance of methods were evaluated on transcriptomic and phospho-proteomic perturbation experiments and showed that simple linear models (e.g. ULM, MLM, etc.) outperform GSEA, whilst ORA performs similarly well [@BadiaIMompel2022]. Additionally, it is worth noting that pathway analysis was originally developed for gene expression data and even tough it was adapted to metabolomics data, it often produces misleading or nonsensical results due to the unique characteristics and constraints of metabolites compared with genes [@Lee2025]. In the next paragraph we will discuss those pitfalls in more detail.

***1. Annotated metabolites and coverage in the prior knowledge***
Different to gene names, there are no standardized names for metabolites. Indeed, there are different databases that collect metabolite information and assign a metabolite ID to each entry, as for example, the Human Metabolome Database (HMDB) with HMDB IDs [@Wishart_2007] or the Kyoto Encyclopedia of Genes and Genomes (KEGG) with KEGG IDs [@Kanehisa2017]. Additionally, those databases often include multiple entries for the same metabolite with different degrees of ambiguity [@Pham2019], as for example KEGG, which includes entries for L-Alanine, D-Alanine and Alanine. Since experimentally different degrees of ambiguity are needed due to the machine sensitivity, where for example stereoisomers are not distinguished [@Dias2016]. Yet, standard metabolite-sets used as prior knowledge often contain one specific metabolite ID, which can hinder mapping to the detected data [@Schmidt2025]. Consequently,  if metabolite detection is unspecific due to machine sensitivity, it is important to assign all possible metabolite IDs to ultimately increase the overlap with metabolite-sets of interest [@Schmidt2025]. 
Biocrates, whose kit was used to detect the example study, does an excellent job since they provide all possible identifiers for each trivial metabolite name (Table 1). Yet, if this is not the case and only one metabolite ID type is available or within an ID type only one ID was assigned, it is important to translate metabolite IDs, quantify ambiguities and assign potential other IDs to the detected metabolites, which can be done using the R package MetaProViz [@Schmidt2025].

<!-- ::: {.table} -->

Table 1: **Preview of selected metabolites to showcase ChEBI and HMDB information provided by Biocrates.** 

| Trivial Name                | Class             | ChEBI                     | HMDB                    |
|-----------------------------|-------------------|---------------------------|-------------------------|
| *Ala*                       |Aminoacids         |15570, 16449, 16977, 76050 |HMDB0000161, HMDB0001310 |
| *C0*                        |Acylcarnitines     |11060, 16347, 17126        |HMDB0000062              |
| *Cer d16:1/18:0*            |Ceramides          |None                       |None                     |
| *DG 16:0_20:3*              |Diacylglycerols    |84402                      |HMDB0007111              |


: width="100%"

: tbl-colwidths="28,17,17,21,17"

<!-- ::: -->

Looking into the example metabolites from Table 1 it becomes clear that for some metabolites multiple ChEBI and HMDB IDs are available, whilst for other metabolite those databases have no entry. To systematically understand the metabolite ID coverage, we visualize the available metabolite IDs for the detected metabolites in our example data using an upset plot (Figure 4a).
```{r}
#| eval: true
#| echo: false
#| message: false
#| warning: false

# Load biocrates IDs:
FeatureMetadata_Biocrates <- MetaProViz::ToyData(Data="BiocratesFeatureTable")

# Add IDs to our measured data
SettingsFile_Metab <-  merge(x= as.data.frame(SummarizedExperiment::rowData(metabolomics))[,c(1:2)],
                             y= FeatureMetadata_Biocrates[,c(2:3,5,7,11)],
                             by.x="feature_original",
                             by.y="TrivialName_Prior2023",
                             all.x=TRUE)

#Plot Upset plot: (will be updated in MetaProViz soon, as we are adding it to export)
Plotdf <- data.frame(
  trivname = SettingsFile_Metab$feature_original,
  CHEBI = as.integer(!is.na(SettingsFile_Metab$CHEBI)),
  HMDB  = as.integer(!is.na(SettingsFile_Metab$HMDB)),
  LIMID = as.integer(!is.na(SettingsFile_Metab$LIMID))
)%>%
  mutate(
    None = as.integer(rowSums(across(c(CHEBI, HMDB, LIMID))) == 0),
    Class = as.factor(SettingsFile_Metab$Class)  # or however you access the Class column
  )

Fig4A <- MetaProViz:::GenerateUpset(df = Plotdf,
                                    class_col = "Class",
                                    intersect_cols = c("LIMID", "HMDB", "CHEBI", "None"),
                                    plot_title = "Biocrates IDs available for the detected metabolites.",
                                    palette_type = "polychrome",
                                    output_file = NULL)

```

It becomes apparent that for some metabolite classes only certain metabolite ID types are available (Figure 4a). This means, if for example Reactome [@Gillespie_2022], which uses chEBI IDs is used for pathway enrichment analysis, only a small subset of the detected metabolites will be included, which needs to be taken into consideration when interpreting the results biologically. 

In order to ensure the maximum overlap between measured metabolites and prior knowledge of choice we recommend meticulous assignment of metabolite IDs to the detected metabolites. Lastly, given the incomplete coverage in MS-based metabolomics and hence the low coverage of the prior knowledge, it is important to take this into consideration for statistical testing. Indeed, for metabolomics methods such as ORA were discussed to perform well if used correctly, since ORA offers the ability to restrict the background to the detected metabolites (assay-specific background) [@Wieder2021].

***2. Available prior knowledge for metabolites***
Metabolite sets for classical mappings can be found or extracted from many different databases such as KEGG, Reactome or WikiPathways to name a few, and there are also databases like RampDB that have merged these resources to provide an integrated database [@Braisted2023]. These classical databases mainly carry information about the metabolic pathway and the metabolites that are assigned to them. Here it is important to acknowledge that the same metabolite will be part of multiple pathways and often include unspecific metabolites like cofactors, ions, H2O or CO2, which can not be detected with a classical mass-spec setups. The reason for this is that many metabolic pathways originate from genome scale metabolic models, which cover the whole reaction, and have not been adapted for the coverage in a metabolomics experiment. Hence, we recommend to exclude those metabolites prior to performing enrichment analysis. Apart from pathway-metabolite sets, chemical class-metabolite sets can be used for classical mapping to understand if certain compound classes are affected in the comparison of interest [@REF]. 
Both of those examples, pathway-metabolite sets and chemical class-metabolite sets, are not able to capture if the changed metabolites have other functions apart from being educts and products of enzymatic reactions. Indeed, metabolites can also act as signalling molecules by binding to receptors, which is not captured in those metabolite sets [@Farr&Dimitrov2024]. Therefore, we have developed a new database called MetalinksDB, which contains information about metabolite-receptor and metabolite-transporter interaction, as well as weather the binding of the metabolite to a receptor is activating or inhibiting [@Farr&Dimitrov2024]. Using MetalinksDB can not only enables combined pathway enrichment analysis on metabolite-gene sets or built a network of those interactions, but also adds biological value by highlighting the signalling potential of the metabolites [@Schmidt2025].
Lastly, the prior knowledge network ocean includes a manually curated reduced model of human metabolism based on redHuman of Recon 3D and has developed a specific adaptation of weighted mean, using enzyme distance to weight interaction, to perform enrichment analysis [@Sciacovelli&Dugourd2022]. Hence, ocean explores coordinated deregulations of metabolite abundances to generate a metabolic footprint for each metabolic enzyme and ultimately define bottlenecks [@Sciacovelli&Dugourd2022].

To use these different databases the original resource can be downloaded, but there are also more convenient solutions. Indeed, RaMP-DB has combined many classical resources trying to account for miss-mapping between the original resources by using molecular weight [@REF]. Additionally, RaMP-DB has used ClassyFire to assign chemical classes to the metabolites, which can be used for classical mapping [@Braisted2023]. Similarly, Omnipath a database of molecular biology prior knowledge combining data from more than 1000 resources [@Türei2016, @Türei2021] has recently been extended and incorporated many database clients containing metabolites and utils for metabolite identifier translation (https://metabo.omnipathdb.org/). MetaProViz includes a collection of annotated metabolite sets usable for classical mapping called Metabolism Signature Database (MetSigDB), which includes classical pathway-metabolite sets, chemical class-metabolite sets, MetalinksDB metabolite-receptor and transporter sets[@Schmidt2025]. Moreover, MetaProViz uses the Cosmos enzyme-metabolic reaction prior knowledge network [@Dugourd_2021] to convert any gene set to gene-metabolite sets, which can be used to perform combined enrichment analysis [@Schmidt2025]. If no metabolomics data are available, but the focus is still on metabolic pathways, we recommend using the gene-sets generated by Gaude et. al., which are specific to include metabolic enzymes and enable weighting for enzymes that are unique for a pathway compared to enzymes present in multiple pathways [@Gaude2016].

***3. Biological interpretation of the results***
The biological interpretation of the results is crucial and should be done in the context of the biological question and the experimental design. Here it is important to take into account multiple points:

1. Understand the limitations of the prior knowledge used, such as potential biases in the underlying original resources. For example, if the prior knowledge is based on a specific database, it may not capture all relevant metabolites or pathways. This can lead to biased results and misinterpretation of the data. Moreover, the  choice of prior knowledge dictates the biological question one can address as discussed in the paragraph above for classical pathway-metabolite sets, chemical class-metabolite sets or receptor-metabolite sets.

2. Understand the coverage of your experimental data in the prior knowledge resource. For example, if the prior knowledge only includes metabolites that are available on HMDB, this would in turn exclude many detected ceramides, glycosylceramides and triglycerols, whilst using ChEBI IDs would exclude almost all triacylglycerols (Fig. 4a-d). Here triacylglycerols include 226 metabolites of which only 11 have a ChEBI ID (Fig.4d). Hence, if the results of the enrichment analysis do not show those pathways as significantly altered, it does not mean that they are not altered, but rather that the prior knowledge used is not able to capture them. Consequently, it is important not to over-interpret the importance of the most altered pathways, but rather to use them as a starting point for further investigation not ignoring the detected metabolites that were not covered by the prior knowledge resource. Otherwise, combining multiple prior knowledge resources to increase the coverage of the detected metabolites is also an option. Lastly, it is important to check how many metabolites of e.g. a pathway in the prior knowledge have been detected. For example if a pathway only contains 20 metabolites and 13 of them are detected (65%), this is a stronger evidence than detecting 40 metabolites of a pathway containing 110 metabolites (36%). 

```{r}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| results: false

InputData <- DMA_Res_wilcox[["DMA"]][["TU _vs_ NAT"]]%>%
  tibble::column_to_rownames("Metabolite")%>%
  dplyr::filter(class %in% c("Ceramides", "Triacylglycerols", "Glycosylceramides"))
                    
SettingsFile_Metab <-  merge(x= as.data.frame(SummarizedExperiment::rowData(metabolomics))[,c(1:2)],
                             y= FeatureMetadata_Biocrates[,c(2:3,5,7,11)],
                             by.x="feature_original",
                             by.y="TrivialName_Prior2023",
                             all.x=TRUE)%>%
  mutate(
    combined_id = case_when(
      !is.na(CHEBI) & !is.na(HMDB) & !is.na(LIMID) ~ "CHEBI-HMDB-LIMID",
      !is.na(CHEBI) & !is.na(HMDB) &  is.na(LIMID) ~ "CHEBI-HMDB",
      !is.na(CHEBI) &  is.na(HMDB) & !is.na(LIMID) ~ "CHEBI-LIMID",
      is.na(CHEBI) & !is.na(HMDB) & !is.na(LIMID) ~ "HMDB-LIMID",
      !is.na(CHEBI) &  is.na(HMDB) &  is.na(LIMID) ~ "CHEBI",
      is.na(CHEBI) & !is.na(HMDB) &  is.na(LIMID) ~ "HMDB",
      is.na(CHEBI) &  is.na(HMDB) & !is.na(LIMID) ~ "LIMID",
      TRUE ~ "NONE"
    )
  ) %>%
  group_by(Class, combined_id) %>%
  mutate(n_combination = n(),
    combined_label = paste0(combined_id, ": ", n_combination)) %>%
  ungroup()%>%
  tibble::column_to_rownames("feature")
  

# Plot: 
Res <- MetaProViz::VizVolcano(InputData=InputData ,
                       SettingsInfo= c(individual="Class", color="combined_label"),
                       SettingsFile_Metab= SettingsFile_Metab,
                       ylab ="-log10(p.adj)",
                       yCutoff = 0.1,
                       PlotName = "Wilcoxon & FDR")

Fig4B <- Res[["Plot_Sized"]][["Ceramides"]]
Fig4C <- Res[["Plot_Sized"]][["Glycosylceramides"]]
Fig4D <- Res[["Plot_Sized"]][["Triacylglycerols"]]  
```


3. Understand the impact of metabolite-set inflation and deflation on the results. Indeed, due to the different metabolite ID types in combination with the metabolite ID coverage in metabolite-sets, it sometimes is required to translate metabolite IDs from one identifier type to another. For example, in the biocrates example data no KEGG IDs are available, yet if KEGG pathways are used for enrichment analysis, either the KEGG pathways or the metabolite IDs of the data have to be translated, which can lead to one-to-none, one-to-one, one-to-many and many-to-many mappings between the metabolite ID types. In turn, this mapping ambiguity can lead to inflation or deflation of the metabolite-set size, which needs to be taken into account when interpreting the results. To translate between metabolite IDs and quantify the mapping ambiguities tools like the R-package MetaProViz can be used [@Schmidt2025]. In case of combined gene-metabolite data, where combined enrichment analysis can be performed, it is crucial that the difference in feature space between metabolomics and transcriptomics or proteomics, since this can underestimate the altered metabolites.

4. Understand how the choice of the statistical method used for enrichment analysis influences the interpretation of the results (see discussion above). Here it is also important to note that the chosen thresholds for e.g. up-regulated metabolites will affect the ORA results as it affects the basket size. The choice of ranking by Log2FC versus ranking by t-value, which is a combination of change and statistics, will alter the GSEA results.

5. Understand if performing enrichment analysis will be beneficial for the biological question at hand. Since classical pathway analysis was originally developed to analyse high-throughput gene expression data, yet metabolites are not genes [@Lee2025].  In general, when a pathway is up-regulated in the context of genes, all the genes involved in that pathway produce more copies, whilst metabolite levels show lower coherence during regulation since the up-regulation may increase the abundance of the end-product but not of the metabolite intermediates[@Lee2025]. Moreover, pathway enrichment results from metabolite measurements in urine or blood plasma can not be directly interpreted as altered pathway biosynthesis, since the enzymes required for many pathways are not present and it is rather to be thought that these metabolites were released into those fluids from organs or blood cells [@Lee2025]. Here it is also relevant to consider that the origin of the pathways is based on well-established characterization, which has been discussed to hinder novel biological insights [@Hu_2025]. Hence, we would recommend to always perform literature searches for the top/bottom altered metabolites, especially if they are not well studied. Noteworthy, beyond metabolic pathways, metabolites play an active role in gene-regulation by for example binding to receptors and elucidating signalling cascades [@Baker_2023 , @Farr&Dimitrov2024].

```{r}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| results: false

# Example MetalinksDB to combine with proteomics


```

To sum up, in order to interpret a metabolomics experiment in its entirety, it is important to understand the limitations of the prior knowledge used, the coverage of the experimental data in the prior knowledge resource, the impact of metabolite-set inflation and deflation on the results and the choice of statistical method used for enrichment analysis. Researchers should not solely rely on pathway analysis and often it is more beneficial to perform literature searches for the top/bottom altered metabolites, extract metabolite receptors that could be targeted by the altered metabolites or transporters that could release the metabolites into the environment. All of this will require close collaboration between data scientists and experimentalist in the future.

```{r figure_4}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| results: false


row1 <- plot_grid(Fig4A, ncol = 1, labels = c("A"))
row2 <- plot_grid(Fig4B, Fig4C, Fig4D, ncol = 3, labels = c("B", "C", "D"))
#row3 <- plot_grid(Fig3E, Fig3F, Fig3G, ncol = 3, labels = c("E", "F", "G"))
g <- plot_grid(row1, row2, ncol = 1, rel_heights = c(2, 1))

ggsave(filename = "figure/figure4.jpg", 
       plot = g, 
       device = "jpg", 
       dpi = 600, 
       width = 16, 
       height = 14) 
```

## Experimental network analysis using statistical methods

Statistical analysis uncovers pairwise relationships between metabolites and/or proteins within each modality. The analysis can be done to assess the co-occurence, association, or regulation of features within a dataset without any prior knowledge. The calculation of pairwise coefficients is often done via the Pearson or Spearman correlation calculation methods, which are useful for exploratory analysis to identify correlated feature pairs, suggesting potential biological relationships or shared pathways (guilt-by-association principle). Other methods may be more suitable depending on the research question, such as ARACNE (Algorithm for the Reconstruction of Accurate Cellular Networks), Bayesian network learning, CLR (Context Likelihood of Relatedness), GGM (Gaussian Graphical model),  linear regression using LASSO (Least Absolute Shrinkage and Selection Operator) regularization, partial Pearson correlation, Random Forest, or partial Spearman correlation. In the resulting network, vertices represent features, such as metabolites or proteins, while edges represent the statistical relationship between them, such as correlation coefficients.

We will showcase the creation of statistical networks based on the implemented functionality of the MetNet [@Naake2019] package.

1.  Choose an algorithm based on the data and biological question:

- *ARACNE* represents an mutual information (MI)-based approach that removes indirect interactions using data processing inequality. The algorithm should be used when aiming to extract the most relevant direct feature interactions from MI-based networks. It is more accurate than pure MI-based methods by reducing false positives by filtering indirect edges. The algorithm requires large sample sizes to estimate MI accurately.
- *Bayesian network learning*  infers regulatory networks by modeling conditional dependencies between genes using a probabilistic graphical model. The algorithm may be used when prior biological knowledge is available and probabilistic modeling is needed. The algorithm handles noise well, allows for causal inference, and is able to handle missing data. It may be, however, computationally expensive, requires strong prior assumptions, and may be sensitive to sample size.
- *CLR* uses MI to infer regulatory relationships, normalizing interactions for each feature. The algorithm works well when working with noisy or indirect regulatory influences. It improves MI-based inference by reducing false positives and performs well for global network structures. The algorithm assumes that regulatory interactions follow a Gaussian-like distribution, which may not always be true.
- *GGM* estimates a sparse precision matrix (inverse covariance) to infer direct interactions. The algorithm can be used when assuming a multivariate Gaussian structure in expression data. It identifies direct dependencies and avoids indirect correlations. The algorithm requires large sample sizes to obtain estimates accurately.
-  *Linear regression using LASSO regularization* identifies regulatory network by selecting a sparse set of predictor features for each target feature, thereby reducing the number of spurious edges in a network. The algorithm handles high-dimensional data well, reduces overfitting, and provides an interpretable sparse network. The algorithm assumes a linear relationship, may struggle with correlated predictors (features with similar expression profiles, and may be computationally expensive.
-   *Pearson correlation* measures linear relationships between feature expression/abundance profiles. The algorithm should be used when linear dependencies exist between feature expression/abundance levels. The algorithm is simple, computationally efficient, and easily interpretable. It does not capture non-linear relationships and is sensitive to noise.
-   *Partial Pearson correlation* identifies direct regulatory interactions by controlling for indirect effects. It can be used when confounding influences in a network must be corrected. The algorithm removes indirect correlations, providing clearer direct relationships. It requires large sample sizes to compute reliably.
-   *Random forest* infers regulatory network using feature importance scores from an ensemble of decision trees. The algorithm works well when dealing with non-linear relationships and heterogeneous data. It captures complex dependencies, is robust to noise, and does not assume a specific distribution. The algorithm can be computationally expensive and may overfit if not tuned properly.
-   *Spearman correlation* captures monotonic (rank-based) relationships between feature expression/abundance levels. The algorithm may be used when the data does not follow a normal distribution or when looking for ranked relationships. It is robust to outliers and captures non-linear relationships. The algorithm is less powerful for detecting direct gene interactions.
- *Partial Spearman correlation* is similar to partial Pearson correlation but for rank-based dependencies. The algorithm controls for confounding variables while dealing with non-linear relationships. It handles non-linearity and indirect interactions. It is less widely used in regulatory network inference and may be computationally demanding.

2.  Calculate the adjacency matrix using the `MetNet` [@Naake2019]. Specify the algorithms to be run. (optional) Prior to running the model, check if the model tolerates missing values (**NA**) and impute values if needed. (optional) Apply the Benjamini-Hochberg method for p-value adjustment for Pearson and Spearman correlation. The result of each algorithm is stored in a respective assay of the *AdjacencyMatrix* object, an extension of the *SummarizedExperiment* class. Note, that depending on the chosen algorithm, the adjacency matrices are not necessarily symmetric.

```{r network_statistical}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| results: false
stat_adj <- imputeAssay(assay(metabolomics), "MinDet") |>
    statistical(model = c("pearson", "spearman"), 
        p.adjust = "BH")
```

3.  Examine the distribution of similarity coefficients and p-values, if available.

```{r}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| results: false
#| fig-show: hide
## coefficients
coefs <- assay(stat_adj, "pearson_coef")[lower.tri(assay(stat_adj, "pearson_coef"))]
df <- data.frame(coefs = coefs)
    
png("figure/figure5_histogram_coef.png")
ggplot(df, aes(x = coefs)) +
    geom_histogram(fill = "steelblue", color = "black" ,alpha = 0.7) +
    labs(x = "Pearson correlation coefficients", y = "frequency") +
    theme_bw()
dev.off()
    
## p-values
pvals <- assay(stat_adj, "pearson_pvalue")[lower.tri(assay(stat_adj, "pearson_pvalue"))]
df <- data.frame(pvals = pvals)

png("figure/figure5_histogram_pvalues.png")
ggplot(df, aes(x = pvals)) +
    geom_histogram(fill = "#DC143C", color = "black" ,alpha = 0.7) +
    labs(x = "adjusted p-values", y = "frequency") +
    theme_bw()
dev.off()
```

4.  (optional) Retain highly correlating values (*choice of user*). Feature pairs with low
    coefficients will be regarded as noise and removed from the adjacency 
    matrix. 

```{r network_threshold}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| results: false
args_thr <- list(
    filter = "abs(pearson_coef) > 0.8 & pearson_pvalue < 0.05 & abs(spearman_coef) > 0.7 & spearman_pvalue < 0.05")
stat_adj_thr <- threshold(am = stat_adj, type = "threshold", args = args_thr)
```

5.  Create a graph from the consensus adjacency matrix. Specify the graph 
characteristics, e.g., if the graph should be unweighted *or* weighted or 
undirected *or* directed.

```{r network_graph}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| results: false
g <- graph_from_adjacency_matrix(assay(stat_adj_thr, "consensus"), 
    weighted = FALSE, mode = "undirected")
```

6.  (optional) Remove the singleton components and obtain the induced subgraph.

```{r network_graph_filtering}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| results: false
components_g <- igraph::components(g)
components_keep <- which(components_g$csize > 1)
V_keep <- names(components_g$membership)[
    components_g$membership %in% components_keep]
g_keep <- induced_subgraph(g, V_keep)
```

5.  Visualize network. Results from upstream analyses can be mapped to the network. As an example, the weights of latent factor 1 of the trained MOFA model can be used for colouring of the network vertices. Alternatively, we can use logFC of the differential expression analysis.

```{r network_visualisation}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| results: false
#| fig-show: hide
attributes_df <- get_weights(MOFAobject)$metabolomics |>
    as.data.frame() 
attributes_df <- cbind(name = rownames(get_weights(MOFAobject)$metabolomics), 
    attributes_df)
g_keep <- add_vertex_attributes(g = g_keep, attributes = attributes_df)

## create a color gradient: Blue (low) → White (neutral) → Red (high)
colors <- colorRampPalette(c("blue", "white", "red"))(100)  # 100 gradient steps

## normalize Factor1 values to fit within the color gradient index (1 to 100)
factor1_scaled <- round(scales::rescale(V(g_keep)$Factor1, to = c(1, 100)))

## assign colors based on scaled Factor1 values
V(g_keep)$color <- colors[factor1_scaled]

png("figure/figure5_network_factor1.png")
graphics::layout(matrix(c(1, 2), nrow = 2), heights = c(12, 1)) 
plot(g_keep, vertex.label = NA, vertex.size = 3, legend = TRUE)

min_val <- round(min(V(g_keep)$Factor1), digits = 2)
max_val <- round(max(V(g_keep)$Factor1), digits = 2 )

## plot the color legend
image.plot(legend.only = TRUE, add = FALSE, legend.lab = "MOFA Factor1 values", 
    horizontal = TRUE, col = colors, zlim = c(min_val, max_val), 
    legend.width = 1, legend.shrink = 0.2,
    axis.args = list(at = c(min_val, 0, max_val), 
    labels = c(min_val, "0", max_val), 
    cex.axis = 0.8))
dev.off()
```

6.  Choose and apply module determination algorithm (*choice of user*). To detect substructures in the graph, module determination algorithms can be applied on the graph. Common algorithms are (*igraph* implementation in brackets, see Table 1 for further details):
- *Community structure detection based on edge betweenness* (*cluster_edge_betweenness*) detects communities by iteratively removing edges with the highest betweenness centrality, splitting the network into distinct modules. It works well for small graphs, provides a clear hierarchical structure. The algorithm may be computationally expensive for large networks.
- *Community structure detection via greedy optimization of modularity* (*cluster_fast_greedy*) uses hierarchical agglomeration to merge vertices into communities, optimizing the modularity at each step. The algorithm works efficiently for large graphs, but 
is less effective for networks with overlapping communities.
- *Community structure detection algorithm based on interacting fluids* (*cluster_fluid_communities*) uses label propagation with a fixed number of communities, allowing them to expand dynamically based on node density. The algorithm is fast. The algorithm requires to predefine the number of communities and may yield inconsistent results across runs.
- *Community structure detection based on Infomap* (*cluster_infomap*) works by simulating random walks on the network and compressing the path description to reveal the best modular structure. The idea is that if a network has well-defined communities, a random walker will spend more time inside a community before jumping to another one. The algorithm captures this by trying to minimize the description length of the walk, effectively grouping vertices that are frequently visited together. The algorithm may be computationally demanding and is sensitive to parameter choices.
- *Community structure detection detection based on propagating labels* (*cluster_label_prop*) assigns labels to vertices, which spread iteratively based on majority voting until convergence. The algorithm scales linearly with the number of vertices and is, thus, works well on large networks. Due to the random nature of initiation, the results can be unstable and inconsistent for certain networks.
- *Community structure detection detection based on leading eigen* (*cluster_leading_eigen*) uses spectral clustering by computing the leading eigenvectors of a modularity matrix to find communities. The algorithm may be computationally expensive for large networks.
- *Community structure detection using the Leiden algorithm* (*cluster_leiden*) is a faster version of the Louvain algorithm and yields more accurate solutions. It iteratively refines partitions to ensure well-connected communities by optimizing either the modularity or the Constant Potts model, which does not suffer from the resolution limit. The algorithm may have difficulties finding small communities.
- *Community structure detection by multi-level optimization of modularity* (*cluster_louvain*) optimizes modularity by merging hierarchically small communities into larger ones in a greedy manner. The algorithm scales well to large networks. It may produce different results on each run and may has difficulties finding small communities.
- *community structure detection using the maximum modularity* (*cluster_optimal*) calculates the optimal community structure by maximizing the modularity over all possible partitions. The algorithm guarantees the best modularity score, but is computationally expensive for large network.
- *Community structure detection based on statistical mechanics* (*cluster_spinglass*) models community detection as an energy minimization problem via a spin-glass model and simulated annealing. The algorithm is computationally expensive for large networks and may be sensitive to parameter settings.
- *Community structure detection based on short random walks* (*cluster_walktrap*) simulates random walks on the network to detect community structure, assuming vertices within the same community are more likely to be visited in one walk. The algorithm may be slow for large networks.


<!-- ::: {.table} -->

Table 2: **Compatibility of community detection algorithms with network properties** 

| algorithm                   | undirected   | directed     | unweighted   | weighted     |
|-----------------------------|--------------|--------------|--------------|--------------|
| *cluster_edge_betweenness*  |$$\checkmark$$|$$\checkmark$$|$$\checkmark$$|$$\checkmark$$|
| *cluster_fast_greedy*       |$$\checkmark$$|  $$\times$$  |$$\checkmark$$|$$\checkmark$$|
| *cluster_fluid_communities* |$$\checkmark$$|  $$\times$$  |$$\checkmark$$|  $$\times$$  |
| *cluster_infomap*           |$$\checkmark$$|$$\checkmark$$|$$\checkmark$$|$$\checkmark$$|
| *cluster_label_prop*        |$$\checkmark$$|$$\checkmark$$|$$\checkmark$$|$$\checkmark$$|
| *cluster_leading_eigen*     |$$\checkmark$$|  $$\times$$  |$$\checkmark$$|$$\checkmark$$|
| *cluster_leiden*            |$$\checkmark$$|  $$\times$$  |$$\checkmark$$|$$\checkmark$$|
| *cluster_louvain*           |$$\checkmark$$|  $$\times$$  |$$\checkmark$$|$$\checkmark$$|
| *cluster_optimal*           |$$\checkmark$$|$$\checkmark$$|$$\checkmark$$|$$\checkmark$$|
| *cluster_spinglass*         |$$\checkmark$$|  $$\times$$  |$$\checkmark$$|$$\checkmark$$|
| *cluster_walktrap*          |$$\checkmark$$|  $$\times$$  |$$\checkmark$$|$$\checkmark$$|  

: width="100%"

: tbl-colwidths="28,17,17,21,17"

<!-- ::: -->


```{r network_cluster}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| results: false

attributes_df <- data.frame(
    infomap = membership(cluster_infomap(g_keep, e.weights = E(g_keep))))
attributes_df <- cbind(name = rownames(attributes_df), 
    attributes_df)
g_keep <- add_vertex_attributes(g = g_keep, attributes = attributes_df)

## assign colors based on scaled Factor1 values
V(g_keep)$color <- as.factor(V(g_keep)$infomap)

png("figure/figure5_network_infomap.png")
plot(g_keep, vertex.label = NA, vertex.size = 3)
dev.off()
```

```{r figure_5}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| results: false
#| fig-show: hide
## load images
gg_hist_coef <- image_read("figure/figure5_histogram_coef.png")
gg_hist_pvalues <- image_read("figure/figure5_histogram_pvalues.png")
gg_network_factor1 <- image_read("figure/figure5_network_factor1.png")
gg_network_infomap <- image_read("figure/figure5_network_infomap.png")

## convert images to ggplot objects
gg_hist_coef <- ggdraw() + draw_image(gg_hist_coef)
gg_hist_pvalues <- ggdraw() + draw_image(gg_hist_pvalues)
gg_network_factor1 <- ggdraw() + draw_image(gg_network_factor1)
gg_network_infomap <- ggdraw() + draw_image(gg_network_infomap)

g <- plot_grid(gg_hist_coef, gg_hist_pvalues, 
                gg_network_factor1, gg_network_infomap, 
     ncol = 2, rel_widths = c(1, 1), labels = "AUTO")
ggsave(filename = "figure/figure5.jpg", plot = g, device = "jpg", dpi = 600)
```


# Conclusion

This chapter demonstrated how to perform metabolomics data analysis in *R* using a network-based approach. By integrating tools for preprocessing, differential analysis, pathway and correlation analysis, and data integration, *R* provides a comprehensive framework for deriving biological insights from metabolomics and proteomics datasets. Parameters such as thresholds, normalization methods, and correction techniques must be carefully chosen to ensure the robustness and reliability of the results. 

# References


# Figure Captions/Table Captions/Tables


Figure 1: Assessment of log-transformation of proteomics dataset on quality metrics.
A: Mean-sd plot of raw data. B: Mean-sd plot of transformed data. 
Log-transformation controls heteroskedasticity.
C: Matrix of Euclidean distances between samples of raw data. 
D: Matrix of Euclidean distances between samples of transformed data.
E: PCA of raw data. F: PCA of transformed data. Log-transformation increases
variation on prinpical component 1 and 2.

Figure 2:

Figure 3:
A-B) Data distribution for each metabolite abundance of A) Normal Adjacent Tumour (NAT) and B) Tumour (TU). 
C-H) Volcano plots of differential analysis comparing TU versus NAT using either the t-test and FDR adjustment (C) or the Wilcoxon rank-sum test and FDR adjustment (D-H). E) is colour coded based on the metabolite class as defined by Biocrates, whilst F-H) are selected metabolite panels based on Biocrates class.

Figure 4:
A) Upset plot of the metabolites IDs of the detected metabolites in the biocrates example dataset colour coded for the Biocrates defined metabolite classes.
B-C) Volcano plots of differential analysis comparing TU versus NAT using the Wilcoxon rank-sum test and FDR adjustment, selecting for metabolite panels based on Biocrates class and colour coding for the available metabolite ID types.

Figure 5:
